{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#aerofusion-autonomous-blimp-navigation-and-sensor-integration-platform","title":"AeroFusion: Autonomous BLIMP Navigation and Sensor Integration Platform","text":""},{"location":"#1-repository-structure-review","title":"1. Repository Structure Review","text":"<p>Before diving into the project details, we reviewed the cloned template  to understand how the folder structure translates to the website. We removed all markdown pages except for index.md to prepare the report. This clean setup ensures that our final website only displays the main home page, keeping the structure clear and uncluttered.</p>"},{"location":"#2-home-page","title":"2. Home Page","text":"<p>Project Name: </p> <p>AeroFusion: Autonomous BLIMP Navigation and Sensor Integration Platform</p> <p>Team Number: </p> <p>Team 03</p> <p>Team Members: </p> <ul> <li>Nihar Masurkar</li> <li>Prajjwal Dutta</li> <li>Sai Srinivas Tatwik Meesala</li> </ul> <p>Semester and Year: Spring 2025</p> <p>University, Class, and Professor:</p> <ul> <li>University: Arizona State University  </li> <li>Class: RAS 598: Experimentation and Deployment of Robotic Systems</li> <li>Professor: Dr. Daniel Aukes </li> </ul>"},{"location":"#3-project-plan","title":"3. Project Plan","text":""},{"location":"#31-high-level-concept-and-research-question","title":"3.1 High-Level Concept and Research Question","text":"<p>Our project aims to develop an integrated, sensor-driven framework that enables a Biologically-inspired, Lighter-than-air, Instructional, Mechatronics Program (BLIMP) UAV to operate autonomously in dynamic and uncertain environments.</p> <p>The central research question is: \"How effectively can sensor data from various sensors (such as Time-of-Flight Sensor, IMU, Barometer, Camera) be fused together to enhance trajectory planning and autonomous navigation capabilities of a hybrid robotic blimp system in dynamic environments?\"</p> <p> Figure 1: CAD Rendering of Biologically-inspired, Lighter-than-air, Instructional, Mechatronics Program (BLIMP) UAV</p>"},{"location":"#32-sensor-integration","title":"3.2 Sensor Integration","text":""},{"location":"#how-will-you-utilize-sensor-data-collected-from-the-robot-in-your-code","title":"How will you utilize sensor data collected from the robot in your code?","text":"<p>Sensor integration in our project follows a comprehensive, multi-layered approach to ensure that data from each sensor is effectively utilized across all development stages. In our codebase, we've want to implement a modular architecture where sensor data is published on dedicated ROS2 topics. Each sensor continuously streams its specific measurements, allowing for both independent processing and integrated sensor fusion.</p> <p>Our code utilizes a hierarchical data processing pipeline:</p> <ol> <li>Data Acquisition Layer: Handles raw sensor input collection at appropriate sampling rates.</li> <li>Filtering Layer: Applies Kalman filtering and other noise reduction techniques.</li> <li>Fusion Layer: Combines multiple sensor inputs for comprehensive state estimation.</li> <li>Decision Layer: Processes fused data to inform navigation and control decisions.</li> </ol>"},{"location":"#how-will-you-use-sensors-during-testing","title":"How will you use sensors during testing?","text":"<p>During testing, we will validate individual sensor outputs using ROS2 tools like rqt_plot and ros2 topic echo. This methodical approach ensures each sensor is correctly calibrated and operating within expected parameters. Our testing protocol includes:</p> <ul> <li>Sensor Isolation Tests: Verifying each sensor's accuracy and reliability independently.</li> <li>Calibration Verification: Ensuring sensors maintain accuracy across varying conditions.</li> <li>Interference Testing: Identifying potential cross-sensor interference issues.</li> <li>Data Consistency Checks: Confirming consistent readings under controlled conditions.</li> <li>Latency Measurement: Quantifying processing delays for time-sensitive applications.</li> </ul> <p>These tests will be conducted in both controlled indoor environments and field trials, allowing us to observe how sensor data influences the system's stability, localization accuracy, and obstacle detection capabilities in diverse settings.</p>"},{"location":"#how-will-you-use-sensors-in-your-final-demonstration","title":"How will you use sensors in your final demonstration?","text":"<p>In our final demonstration, real-time integration of sensor data will be paramount. Each sensor will play a specific role in enabling autonomous operation:</p> <ul> <li>IMU: Provides orientation control with 6-DoF motion tracking at 200Hz</li> <li>Barometer: Maintains precise altitude control through atmospheric pressure monitoring</li> <li>GPS: Enables robust outdoor localization with meter-level accuracy</li> <li>Raspberry Pi Camera: Supports visual goal detection and obstacle recognition at 30fps</li> <li>Sonar: Offers proximity detection for immediate collision avoidance</li> <li>ToF Sensor (LiDAR): Creates detailed 3D environmental maps for navigation planning</li> </ul> <p> Figure 2: BLIMP Sensor Connection Block Diagram</p> <p>This comprehensive sensor array will drive the autonomous control loops, enabling adaptive trajectory planning and responsive mode switching between manual and autonomous control. The demonstration will showcase how our fusion algorithms synthesize this diverse data to create a cohesive understanding of the environment, allowing the BLIMP to navigate reliably in dynamic, real-world scenarios.</p>"},{"location":"#33-interface-development","title":"3.3 Interface Development","text":""},{"location":"#how-do-you-plan-on-influencing-the-behavior-of-your-robot","title":"How do you plan on influencing the behavior of your robot?","text":"<p>The behavior of our BLIMP will be influenced through a sophisticated dual-mode control strategy that balances user input with autonomous decision-making. We've designed:</p> <ol> <li> <p>Manual Control Mode: Users can precisely adjust the robot's throttle, direction, and altitude via a joystick interface, allowing for direct control during setup, testing, and emergency situations.</p> </li> <li> <p>Autonomous Mode: The robot will employ advanced trajectory planning algorithms based on goal positions detected by the onboard camera. The system will continuously process sensor data to maintain stable flight while navigating toward objectives.</p> </li> <li> <p>Mode Switching: A dedicated joystick button will facilitate seamless transitions between manual and autonomous operations, ensuring operators can quickly regain control if needed.</p> </li> <li> <p>Emergency Override: Safety-critical sensors will trigger automatic responses regardless of current mode, such as obstacle avoidance maneuvers or altitude corrections.</p> </li> </ol> <p>This hybrid approach provides flexibility while maintaining safety and mission objectives.</p>"},{"location":"#what-interfaces-do-you-plan-on-developing-and-using-to-permit-viewing-interaction-and-storing-data","title":"What interfaces do you plan on developing and using to permit viewing, interaction, and storing data?","text":"<p>We are developing a comprehensive GUI based on ROS that serves multiple functions:</p> <ol> <li> <p>Real-time Data Visualization: </p> </li> <li> <p>Sensor data dashboards displaying IMU, barometer, GPS, camera, sonar, and LiDAR outputs</p> </li> <li>Interactive 3D map showing current position, trajectory, and detected obstacles</li> <li> <p>System status indicators including battery levels, motor outputs, and connection quality</p> </li> <li> <p>Control Interface:</p> </li> <li> <p>Mode selection panel (manual/autonomous)</p> </li> <li>Manual control input visualization</li> <li>Goal setting and waypoint management tools</li> <li> <p>Parameter adjustment sliders for tuning control algorithms</p> </li> <li> <p>Data Logging and Analysis:</p> </li> <li> <p>Comprehensive time-stamped data recording of all sensor inputs</p> </li> <li>Performance metrics tracking including stability measures and navigation accuracy</li> <li>Export functionality for post-mission analysis</li> <li>Replay capabilities for reviewing flight data</li> </ol> <p> Figure 3: Mockup Diagram for Sensor Data Visualization in GUI</p> <p>This integrated interface will not only enable immediate interaction during operation but also support detailed post-mission analysis and system refinement.</p>"},{"location":"#34-control-and-autonomy","title":"3.4 Control and Autonomy","text":""},{"location":"#how-do-you-plan-on-connecting-feedback-from-sensors-to-your-controller-in-higher-level-decision-making-processes-or-both","title":"How do you plan on connecting feedback from sensors to your controller, in higher-level decision-making processes, or both?","text":"<p>Our system implements a multi-tiered approach to connect sensor feedback across both low-level control and high-level decision-making processes:</p> <p>Low-Level Control Integration:</p> <ul> <li>Stabilization Loop: IMU and barometer data feed directly into PID controllers that maintain attitude and altitude stability at 100Hz.</li> <li>Motor Control: Sensor-derived error signals adjust motor outputs to compensate for environmental disturbances.</li> <li>Sensor Fusion: Extended Kalman Filter combines IMU, barometer, and GPS data to provide accurate state estimation for the control system.</li> </ul> <p>High-Level Decision Making:</p> <ul> <li>Environment Mapping: LiDAR and camera data construct a dynamic 3D representation of the environment.</li> <li>Path Planning: A* algorithm utilizes the environmental map to generate efficient trajectories around obstacles.</li> <li>Goal Recognition: Computer vision algorithms process camera feeds to identify navigation targets.</li> <li>Mode Selection: Sensor data informs autonomous decisions about when to switch between different control behaviors.</li> </ul> <p>Cross-Level Integration:</p> <ul> <li>Adaptive Parameter Tuning: High-level processes adjust low-level control parameters based on environmental conditions.</li> <li>Hierarchical State Machine: Coordinates transitions between operational modes based on sensor-detected events.</li> <li>Performance Monitoring: Continuous evaluation of control effectiveness informs strategic decision adjustments.</li> </ul> <p>This comprehensive approach ensures sensor data flows seamlessly between tactical (moment-to-moment) and strategic (goal-oriented) layers of the system, enabling both responsive control and intelligent decision-making.</p>"},{"location":"#35-preparation-needs","title":"3.5 Preparation Needs","text":""},{"location":"#what-do-you-need-to-know-to-be-successful","title":"What do you need to know to be successful?","text":"<p>To achieve complete autonomy of the BLIMP, we need comprehensive understanding of several interrelated technical areas:</p> <ol> <li>Sensor Fusion Algorithms: Advanced techniques for integrating heterogeneous sensor data into coherent state estimates.</li> <li>Aerodynamics of Lighter-than-Air Vehicles: Understanding the unique physics governing blimp movement and stability.</li> <li>Trajectory Planning for Non-Holonomic Systems: Specialized path planning considering the BLIMP's movement constraints.</li> <li>ROS2 Architecture: Deep knowledge of the communication patterns, service structures, and component interactions.</li> <li>Computer Vision for Navigation: Algorithms for visual odometry, feature detection, and goal recognition.</li> <li>Control Theory: Mathematical foundations for developing stable control systems in under-actuated platforms.</li> <li>Power Management: Strategies for maximizing flight time through efficient use of limited onboard energy.</li> </ol>"},{"location":"#which-of-those-topics-do-you-need-covered-in-class","title":"Which of those topics do you need covered in class?","text":"<p>For optimal project success, we would benefit from classroom coverage of:</p> <ol> <li>Advanced PID Tuning Strategies: Practical methods for optimizing control parameters in multi-input, multi-output systems.</li> <li>Model Predictive Control (MPC): Theoretical foundations and implementation techniques in ROS for predictive navigation.</li> <li>ROS2 GUI Development: Hands-on instruction for creating intuitive interfaces using RQT or other frameworks.</li> <li>UAV Control Dynamics: Mathematical models specific to lighter-than-air vehicles.</li> <li>Sensor Fusion Techniques: Practical implementation of Extended Kalman Filters for state estimation.</li> <li>Remote System Administration: Efficient use of SSH and remote desktop tools for Raspberry Pi management.</li> <li>Real-time Performance Optimization: Strategies for managing computational resources on embedded platforms.</li> </ol> <p>These topics would provide the technical foundation required to successfully integrate sensor data and advanced control algorithms into a cohesive autonomous system.</p>"},{"location":"#36-final-demonstration","title":"3.6 Final Demonstration","text":""},{"location":"#please-describe-how-will-you-demonstrate-your-work-in-class","title":"Please describe how will you demonstrate your work in class","text":"<p>Our final demonstration will showcase the BLIMP's autonomous capabilities in a comprehensive flight test. The system will:</p> <ol> <li>Take off and achieve stable hovering using barometer and IMU feedback.</li> <li>Identify visual targets placed throughout the demonstration area.</li> <li>Plan and execute an efficient trajectory toward these targets while maintaining altitude.</li> <li>Detect and navigate around obstacles in its path using LiDAR and sonar data.</li> <li>Demonstrate seamless switching between autonomous navigation and manual control.</li> <li>Display real-time sensor data, decision-making processes, and system status via our GUI.</li> </ol> <p>Throughout the demonstration, we will explain the sensor fusion algorithms, control strategies, and decision-making processes occurring in real-time, highlighting how the different components work together to achieve autonomous operation.</p>"},{"location":"#what-resources-will-you-need","title":"What resources will you need?","text":"<p>Facility Requirements:</p> <ul> <li>Large indoor space (such as TECH 162) with at least 10m \u00d7 10m \u00d7 5m clearance.</li> <li>Power outlets for equipment charging and operation.</li> <li>Adequate lighting for camera-based navigation.</li> </ul> <p>Equipment Needs:</p> <ul> <li>Laptop with ROS2 and our custom GUI for system monitoring and control</li> <li>Remote SSH capabilities for system access and troubleshooting</li> <li>Joystick for manual control demonstrations and emergency override</li> <li>Fully assembled BLIMP with all sensors integrated and calibrated</li> <li>Visual markers and obstacles for navigation challenges</li> <li>Backup batteries and emergency repair kit</li> </ul>"},{"location":"#conditions-change-in-any-environment-how-will-your-robot-handle-variability-in-its-environment","title":"Conditions change in any environment. How will your robot handle variability in its environment?","text":"<p>Our system is designed with adaptability as a core principle to handle environmental variability:</p> <p>Atmospheric Changes:</p> <ul> <li>Barometric pressure fluctuations will be compensated through dynamic altitude calculation adjustments</li> <li>Temperature variations affecting sensor readings will be automatically calibrated through periodic reference checks</li> </ul> <p>Lighting Conditions:</p> <ul> <li>The camera vision system employs adaptive exposure and contrast enhancement algorithms</li> <li>Multiple visual recognition approaches (feature-based and neural network) provide redundancy across lighting conditions</li> </ul> <p>Dynamic Obstacles:</p> <ul> <li>Real-time LiDAR scanning continuously updates the environmental map</li> <li>Obstacle avoidance algorithms recalculate trajectories at 5Hz to respond to moving objects</li> <li>Sonar provides redundant close-range detection for last-moment collision avoidance</li> </ul> <p>Wind and Air Currents:</p> <ul> <li>IMU feedback detects unexpected orientation changes caused by air currents</li> <li>PID controllers dynamically adjust motor outputs to compensate for external forces</li> <li>Wind estimation algorithms adapt control parameters based on observed disturbance patterns</li> </ul> <p>This multi-layered approach ensures the BLIMP remains stable and responsive even as environmental conditions change during operation.</p>"},{"location":"#testing-evaluation-plan-please-describe-how-you-can-verify-your-system-is-working-as-expected","title":"Testing &amp; Evaluation plan: Please describe how you can verify your system is working as expected","text":"<p>Our testing and evaluation strategy follows a rigorous multi-stage approach:</p> <p>1. Unit Testing:</p> <ul> <li>Sensor Validation: Each sensor undergoes individual calibration and accuracy verification against known reference values</li> <li>Control Module Testing: Isolated testing of individual control algorithms with simulated inputs</li> <li>Component Integration: Verification of communication between adjacent system components</li> </ul> <p>2. System Integration Testing:</p> <ul> <li>Sensor Fusion Accuracy: Comparison of fused state estimates against ground truth measurements</li> <li>Control Loop Stability: Evaluation of system response to controlled disturbances</li> <li>Resource Utilization: Monitoring of CPU, memory, and network usage during operation</li> </ul> <p>3. Functional Testing:</p> <ul> <li>Hovering Stability: Quantitative measurement of position and attitude variance during stationary flight</li> <li>Navigation Accuracy: Evaluation of path-following precision against predetermined routes</li> <li>Obstacle Avoidance: Success rate in detecting and navigating around obstacles of varying sizes and positions</li> <li>Goal Recognition: Accuracy and response time in identifying and moving toward visual targets</li> </ul> <p>4. Performance Metrics:</p> <ul> <li>Position accuracy: Target &lt; 30cm deviation from planned path</li> <li>Altitude stability: \u00b110cm during stable hover</li> <li>Obstacle detection: 100% detection rate for objects &gt;20cm at distances up to 3m</li> <li>Battery efficiency: &gt;15 minutes of autonomous operation</li> <li>Control latency: &lt;50ms from sensor input to actuator response</li> </ul> <p>These comprehensive testing protocols will verify both individual component functionality and overall system performance against our design specifications.</p>"},{"location":"#37-impact-of-the-work","title":"3.7 Impact of the Work","text":""},{"location":"#provide-a-short-passage-about-the-impact-of-this-work-how-it-will-drive-you-to-learn-new-material-how-it-could-contribute-to-course-development-what-topics-it-will-encourage-learning-of-etc-and-any-other-important-points-not-addressed-above","title":"Provide a short passage about the impact of this work, how it will drive you to learn new material, how it could contribute to course development, what topics it will encourage learning of, etc., and any other important points not addressed above","text":"<p>This project represents a significant opportunity for both technical advancement and educational development:</p> <p>Technical Skill Development:</p> <p>This work will significantly enhance our practical skills by providing hands-on experience with ROS2; a cornerstone platform for modern robotics applications. Thorugh this we will gain invaluable expertise in autonomous navigation, trajectory planning, and real-time sensor fusion for accurate localization. The project's interdisciplinary nature will drive us to bridge the gap between theoretical knowledge and practical implementation, developing skills that are directly transferable to industry applications.</p> <p>Course Development Contributions:</p> <p>The AeroFusion platform will serve as a replicable template for future autonomous blimp initiatives in the course. Our documented development process, including challenges and solutions, will provide a comprehensive case study for sensor-based control systems in unconventional UAVs. The modular nature of our design allows for incremental complexity in future course offerings, where students could focus on improving specific subsystems while maintaining overall functionality.</p> <p>Expanded Learning Horizons:</p> <p>This project will naturally drive us to explore advanced material beyond the standard curriculum, including: - Adaptive control strategies for under-actuated systems - Machine learning approaches to environmental perception - Energy-efficient computing for embedded systems - Human-robot interaction design principles - Robust engineering practices for field-deployable systems</p> <p>Long-Term Impact: Beyond immediate educational benefits, this work lays the foundation for further research into lighter-than-air autonomous vehicles for applications including environmental monitoring, infrastructure inspection, and emergency response. The sensor fusion techniques we develop may inform approaches to similar challenges in other robotic domains, creating knowledge transfer opportunities across the field.</p>"},{"location":"#38-advising","title":"3.8 Advising","text":""},{"location":"#advisor","title":"Advisor:","text":"<p>Dr. Xi Yu (Faculty Advisor)</p> <p>Lab Space: TECH 189</p>"},{"location":"#required-resources","title":"Required Resources:","text":"<ul> <li>Guidance on trajectory planning algorithms.</li> <li>Access to high-precision GPS modules.</li> <li>Assistance in debugging sensor fusion issues.</li> </ul> <p>The team anticipates learning more about multi-sensor fusion algorithms, adaptive control strategies for dynamic environments, and real-time embedded system debugging.</p>"},{"location":"#4-weekly-milestones-weeks-7-16","title":"4. Weekly Milestones (Weeks 7-16)","text":"Week Hardware Software Testing Status 7 Set up core hardware components Create basic user interface Test basic sensors (IMU, GPS) \u2705 8 Test power systems and batteries Develop interface mockups Install and test distance sensors \ud83d\udd04 9 Test motors and speed controllers Connect interface to ROS Begin data collection from sensors \ud83d\udd52 10 Complete all system connections Add real-time sensor displays Calibrate all sensors \ud83d\udd52 11 Conduct first test flight Set up data storage Begin combining sensor data \ud83d\udd52 12 Improve propeller performance Add map view to interface Set up camera processing \ud83d\udd52 13 Refine flight performance Complete control panel Final sensor adjustments \ud83d\udd52 14 Prepare for demonstration Test user interface Optimize autonomous flight paths \ud83d\udd52 15 Finalize presentation materials Complete documentation Prepare for classroom demo \ud83d\udd52 16 Address any hardware issues Fix any software bugs Final system testing and demo \ud83d\udd52 <p>Legend: - \u2705 = Completed - \ud83d\udd04 = In Progress - \ud83d\udd52 = Pending</p>"},{"location":"gantt/","title":"Gantt Chart","text":"<pre><code>%%{init: {'themeVariables': {'opacity': '1'}}}%%\ngantt\n    title Simplified Project Timeline (March - May)\n    dateFormat  YYYY-MM-DD\n\n    section Hardware &amp; Sensors\n    Hardware setup &amp; testing       :done, 2025-03-04, 14d\n    System wiring &amp; integration    :2025-03-25, 7d\n    Flight tests &amp; optimization    :2025-04-01, 14d\n    Final refinements &amp; demo prep  :2025-04-15, 14d\n\n    section Software &amp; Interface\n    GUI design &amp; mockups           :done, 2025-03-04, 14d\n    ROS integration &amp; visualization:active, 2025-03-18, 14d\n    UI development &amp; testing       :2025-04-08, 14d\n    Documentation &amp; finalization   :2025-04-22, 7d\n\n    section Controls &amp; Autonomy\n    Manual control &amp; tuning        :done, 2025-03-04, 14d\n    Stability &amp; trajectory planning:active, 2025-03-18, 14d\n    Autonomy &amp; path following      :2025-04-08, 14d\n    Final optimization &amp; demo      :2025-04-22, 7d</code></pre>"},{"location":"second-page/","title":"Second Page","text":"<p>Things to discuss</p>"},{"location":"static/node_modules/mathjax/","title":"MathJax","text":""},{"location":"static/node_modules/mathjax/#beautiful-math-in-all-browsers","title":"Beautiful math in all browsers","text":"<p>MathJax is an open-source JavaScript display engine for LaTeX, MathML, and AsciiMath notation that works in all modern browsers.  It was designed with the goal of consolidating the recent advances in web technologies into a single, definitive, math-on-the-web platform supporting the major browsers and operating systems.  It requires no setup on the part of the user (no plugins to download or software to install), so the page author can write web documents that include mathematics and be confident that users will be able to view it naturally and easily.  Simply include MathJax and some mathematics in a web page, and MathJax does the rest.</p> <p>Some of the main features of MathJax include:</p> <ul> <li> <p>High-quality display of LaTeX, MathML, and AsciiMath notation in HTML pages</p> </li> <li> <p>Supported in most browsers with no plug-ins, extra fonts, or special   setup for the reader</p> </li> <li> <p>Easy for authors, flexible for publishers, extensible for developers</p> </li> <li> <p>Supports math accessibility, cut-and-paste interoperability, and other   advanced functionality</p> </li> <li> <p>Powerful API for integration with other web applications</p> </li> </ul> <p>See http://www.mathjax.org/ for additional details about MathJax, and https://docs.mathjax.org for the MathJax documentation.</p>"},{"location":"static/node_modules/mathjax/#mathjax-components","title":"MathJax Components","text":"<p>MathJax version 3 uses files called components that contain the various MathJax modules that you can include in your web pages or access on a server through NodeJS.  Some components combine all the pieces you need to run MathJax with one or more input formats and a particular output format, while other components are pieces that can be loaded on demand when needed, or by a configuration that specifies the pieces you want to combine in a custom way.  For usage instructions, see the MathJax documentation.</p> <p>Components provide a convenient packaging of MathJax's modules, but it is possible for you to form your own custom components, or to use MathJax's modules directly in a node application on a server.  There are web examples showing how to use MathJax in web pages and how to build your own components, and node examples illustrating how to use components in node applications or call MathJax modules directly.</p>"},{"location":"static/node_modules/mathjax/#whats-in-this-repository","title":"What's in this Repository","text":"<p>This repository contains only the component files for MathJax, not the source code for MathJax (which are available in a separate MathJax source repository).  These component files are the ones served by the CDNs that offer MathJax to the web.  In version 2, the files used on the web were also the source files for MathJax, but in version 3, the source files are no longer on the CDN, as they are not what are run in the browser.</p> <p>The components are stored in the <code>es5</code> directory, and are in ES5 format for the widest possible compatibility.  In the future, we may make an <code>es6</code> directory containing ES6 versions of the components.</p>"},{"location":"static/node_modules/mathjax/#installation-and-use","title":"Installation and Use","text":""},{"location":"static/node_modules/mathjax/#using-mathjax-components-from-a-cdn-on-the-web","title":"Using MathJax components from a CDN on the web","text":"<p>If you are loading MathJax from a CDN into a web page, there is no need to install anything.  Simply use a <code>script</code> tag that loads MathJax from the CDN.  E.g.,</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See the MathJax documentation, the MathJax Web Demos, and the MathJax Component Repository for more information.</p>"},{"location":"static/node_modules/mathjax/#hosting-your-own-copy-of-the-mathjax-components","title":"Hosting your own copy of the MathJax Components","text":"<p>If you want to host MathJax from your own server, you can do so by installing the <code>mathjax</code> package using <code>npm</code> and moving the <code>es5</code> directory to an appropriate location on your server:</p> <pre><code>npm install mathjax@3\nmv node_modules/mathjax/es5 &lt;path-to-server-location&gt;/mathjax\n</code></pre> <p>Note that we are still making updates to version 2, so include <code>@3</code> when you install, since the latest chronological version may not be version 3.</p> <p>Alternatively, you can get the files via GitHub:</p> <pre><code>git clone https://github.com/mathjax/MathJax.git mj-tmp\nmv mj-tmp/es5 &lt;path-to-server-location&gt;/mathjax\nrm -rf mj-tmp\n</code></pre> <p>Then (in either case) you can use a script tag like the following:</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"&lt;url-to-your-site&gt;/mathjax/tex-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>where <code>&lt;url-to-your-site&gt;</code> is replaced by the URL to the location where you moved the MathJax files above.</p> <p>See the documentation for details.</p>"},{"location":"static/node_modules/mathjax/#using-mathjax-components-in-a-node-application","title":"Using MathJax components in a node application","text":"<p>To use MathJax components in a node application, install the <code>mathjax</code> package:</p> <pre><code>npm install mathjax@3\n</code></pre> <p>(we are still making updates to version 2, so you should include <code>@3</code> since the latest chronological version may not be version 3).</p> <p>Then require <code>mathjax</code> within your application:</p> <pre><code>require('mathjax').init({ ... }).then((MathJax) =&gt; { ... });\n</code></pre> <p>where the first <code>{ ... }</code> is a MathJax configuration, and the second <code>{ ... }</code> is the code to run after MathJax has been loaded.  E.g.</p> <pre><code>require('mathjax').init({\nloader: {load: ['input/tex', 'output/svg']}\n}).then((MathJax) =&gt; {\nconst svg = MathJax.tex2svg('\\\\frac{1}{x^2-1}', {display: true});\nconsole.log(MathJax.startup.adaptor.outerHTML(svg));\n}).catch((err) =&gt; console.log(err.message));\n</code></pre> <p>Note: this technique is for node-based application only, not for browser applications.  This method sets up an alternative DOM implementation, which you don't need in the browser, and tells MathJax to use node's <code>require()</code> command to load external modules.  This setup will not work properly in the browser, even if you webpack it or bundle it in other ways.</p> <p>See the documentation and the MathJax Node Repository for more details.</p>"},{"location":"static/node_modules/mathjax/#reducing-the-size-of-the-components-directory","title":"Reducing the Size of the Components Directory","text":"<p>Since the <code>es5</code> directory contains all the component files, so if you are only planning one use one configuration, you can reduce the size of the MathJax directory by removing unused components. For example, if you are using the <code>tex-chtml.js</code> component, then you can remove the <code>tex-mml-chtml.js</code>, <code>tex-svg.js</code>, <code>tex-mml-svg.js</code>, <code>tex-chtml-full.js</code>, and <code>tex-svg-full.js</code> configurations, which will save considerable space.  Indeed, you should be able to remove everything other than <code>tex-chtml.js</code>, and the <code>input/tex/extensions</code>, <code>output/chtml/fonts/woff-v2</code>, <code>adaptors</code>, <code>a11y</code>, and <code>sre</code> directories.  If you are using the results only on the web, you can remove <code>adaptors</code> as well.</p> <p>If you are not using A11Y support (e.g., speech generation, or semantic enrichment), then you can remove <code>a11y</code> and <code>sre</code> as well (though in this case you may need to disable the assistive tools in the MathJax contextual menu in order to avoid MathJax trying to load them when they aren't there).</p> <p>If you are using SVG rather than CommonHTML output (e.g., <code>tex-svg.js</code> rather than <code>tex-chtml.js</code>), you can remove the <code>output/chtml/fonts/woff-v2</code> directory.  If you are using MathML input rather than TeX (e.g., <code>mml-chtml.js</code> rather than <code>tex-chtml.js</code>), then you can remove <code>input/tex/extensions</code> as well.</p>"},{"location":"static/node_modules/mathjax/#the-component-files-and-pull-requests","title":"The Component Files and Pull Requests","text":"<p>The <code>es5</code> directory is generated automatically from the contents of the MathJax source repository.  You can rebuild the components using the command</p> <pre><code>npm run make-es5 --silent\n</code></pre> <p>Note that since the contents of this repository are generated automatically, you should not submit pull requests that modify the contents of the <code>es5</code> directory.  If you wish to submit a modification to MathJax, you should make a pull request in the MathJax source repository.</p>"},{"location":"static/node_modules/mathjax/#mathjax-community","title":"MathJax Community","text":"<p>The main MathJax website is http://www.mathjax.org, and it includes announcements and other important information.  A MathJax user forum for asking questions and getting assistance is hosted at Google, and the MathJax bug tracker is hosted at GitHub.</p> <p>Before reporting a bug, please check that it has not already been reported.  Also, please use the bug tracker (rather than the help forum) for reporting bugs, and use the user's forum (rather than the bug tracker) for questions about how to use MathJax.</p>"},{"location":"static/node_modules/mathjax/#mathjax-resources","title":"MathJax Resources","text":"<ul> <li>MathJax Documentation</li> <li>MathJax Components</li> <li>MathJax Source Code</li> <li>MathJax Web Examples</li> <li>MathJax Node Examples</li> <li>MathJax Bug Tracker</li> <li>MathJax Users' Group</li> </ul>"}]}