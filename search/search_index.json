{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#aerofusion-autonomous-blimp-navigation-and-sensor-integration-platform","title":"AeroFusion: Autonomous BLIMP Navigation and Sensor Integration Platform","text":""},{"location":"#1-repository-structure-review","title":"1. Repository Structure Review","text":"<p>Before diving into the project details, we reviewed the cloned template to understand how the folder structure translates to the website. To prepare the report, we removed all markdown pages except for index.md. This clean setup ensures that the final website only displays the main home page, keeping the structure clear and uncluttered.</p>"},{"location":"#2-home-page","title":"2. Home Page","text":"<p>Project Name: </p> <p>AeroFusion: Autonomous BLIMP Navigation and Sensor Integration Platform</p> <p>Team Number: </p> <p>Team 03</p> <p>Team Members: </p> <ul> <li>Nihar Masurkar</li> <li>Prajjwal Dutta</li> <li>Sai Srinivas Tatwik Meesala</li> </ul> <p>Semester and Year: Spring 2025</p> <p>University, Class, and Professor:</p> <ul> <li>University: Arizona State University  </li> <li>Class: RAS 598: Experimentation and Deployment of Robotic Systems</li> <li>Professor: Dr. Daniel Aukes </li> </ul>"},{"location":"#3-project-plan","title":"3. Project Plan","text":""},{"location":"#31-high-level-concept-and-research-question","title":"3.1 High-Level Concept and Research Question","text":"<p>The project aims to develop an integrated, sensor-driven framework that enables a Biologically-inspired, Lighter-than-air, Instructional, Mechatronics Program (BLIMP) UAV to operate autonomously in dynamic and uncertain environments.</p> <p>The central research question is: \"How effectively can sensor data from various sensors (such as Time-of-Flight Sensor, IMU, Barometer, Camera) be fused to enhance trajectory planning and autonomous navigation capabilities of a hybrid robotic blimp system in dynamic environments?\"</p> <p> Figure 1: CAD Rendering of Biologically-inspired, Lighter-than-air, Instructional, Mechatronics Program (BLIMP) UAV</p>"},{"location":"#32-sensor-integration","title":"3.2 Sensor Integration","text":""},{"location":"#how-will-you-utilize-sensor-data-collected-from-the-robot-in-your-code","title":"How will you utilize sensor data collected from the robot in your code?","text":"<p>Sensor integration in the project adopts a comprehensive, multi-layered approach to ensure effective utilization of data from each sensor across all development stages. The team aims to implement a modular architecture in the codebase where sensor data is published on dedicated ROS2 topics. Each sensor continuously streams its specific measurements, facilitating both independent processing and integrated sensor fusion.</p> <p>The code utilizes a hierarchical data processing pipeline:</p> <ol> <li>Data Acquisition Layer: Handles raw sensor input collection at appropriate sampling rates.</li> <li>Filtering Layer: Applies Kalman filtering and other noise reduction techniques.</li> <li>Fusion Layer: Combines multiple sensor inputs for comprehensive state estimation.</li> <li>Decision Layer: Processes fused data to inform navigation and control decisions.</li> </ol>"},{"location":"#how-will-you-use-sensors-during-testing","title":"How will you use sensors during testing?","text":"<p>During testing, the team will validate individual sensor outputs using ROS2 tools like rqt_plot and ros2 topic echo. This methodical approach ensures each sensor is correctly calibrated and operating within expected parameters. The testing protocol includes:</p> <ul> <li>Sensor Isolation Tests: Verifying each sensor's accuracy and reliability independently.</li> <li>Calibration Verification: Ensuring sensors maintain accuracy across varying conditions.</li> <li>Interference Testing: Identifying potential cross-sensor interference issues.</li> <li>Data Consistency Checks: Confirming consistent readings under controlled conditions.</li> <li>Latency Measurement: Quantifying processing delays for time-sensitive applications.</li> </ul> <p>These tests will be conducted in both controlled indoor environments and field trials, allowing us to observe how sensor data influences the system's stability, localization accuracy, and obstacle detection capabilities in diverse settings.</p>"},{"location":"#how-will-you-use-sensors-in-your-final-demonstration","title":"How will you use sensors in your final demonstration?","text":"<p>In the final demonstration, real-time integration of sensor data will be paramount. Each sensor will play a specific role in enabling autonomous operation:</p> <ul> <li>IMU: Provides orientation control with 6-DoF motion tracking at 200Hz.</li> <li>Barometer: Maintains precise altitude control through atmospheric pressure monitoring.</li> <li>Raspberry Pi Camera: Supports visual goal detection and obstacle recognition at 30fps.</li> <li>ToF Sensor (LiDAR): Creates detailed 3D environmental maps for navigation planning.</li> </ul> <p> Figure 2: BLIMP Sensor Connection Block Diagram</p>"},{"location":"#gui-mockup-for-ros-based-sensor-visualization","title":"GUI Mockup for ROS-based Sensor Visualization","text":"<p>The ROS-based GUI can be enhanced to include real-time streaming of sensor topics, providing a comprehensive visualization tool for debugging and monitoring system performance. The interface should include:</p> <ul> <li>Live Sensor Data Feeds: Displaying IMU, barometer, and camera outputs.</li> <li>Real-Time 3D Mapping: Visualization of detected obstacles and planned trajectories.</li> <li>System Status Indicators: Battery levels, motor commands, and control mode (manual/autonomous).</li> <li>Logging &amp; Playback: Time-stamped data recording for post-mission analysis.</li> </ul> <p> Figure 3: Initial Idea for a ROS-based GUI to visualize the sensor data</p> <p>These updates ensure that the sensor fusion and autonomy modules operate efficiently, providing real-time insights into the BLIMP's state during both testing and deployment.</p> <p>This comprehensive sensor array will drive the autonomous control loops, enabling adaptive trajectory planning and responsive mode switching between manual and autonomous control. The demonstration will showcase how the fusion algorithms synthesize this diverse data to create a cohesive understanding of the environment, allowing the BLIMP to navigate reliably in dynamic, real-world scenarios.</p>"},{"location":"#33-interface-development","title":"3.3 Interface Development","text":""},{"location":"#how-do-you-plan-on-influencing-the-behavior-of-your-robot","title":"How do you plan on influencing the behavior of your robot?","text":"<p>The behavior of the BLIMP will be influenced through a sophisticated dual-mode control strategy that balances user input with autonomous decision-making. The team has designed:</p> <ol> <li> <p>Manual Control Mode: Users can precisely adjust the robot's throttle, direction, and altitude via a joystick interface, allowing for direct control during setup, testing, and emergencies.</p> </li> <li> <p>Autonomous Mode: The robot will employ advanced trajectory planning algorithms based on goal positions detected by the onboard camera. The system will continuously process sensor data to maintain a stable flight while navigating toward objectives.</p> </li> <li> <p>Mode Switching: A dedicated joystick button will facilitate seamless transitions between manual and autonomous operations, ensuring operators can quickly regain control if needed.</p> </li> <li> <p>Emergency Override: Safety-critical sensors will trigger automatic responses regardless of current mode, such as obstacle avoidance maneuvers or altitude corrections.</p> </li> </ol> <p>This hybrid approach provides flexibility while maintaining safety and mission objectives.</p>"},{"location":"#what-interfaces-do-you-plan-on-developing-and-using-to-permit-viewing-interaction-and-storing-data","title":"What interfaces do you plan on developing and using to permit viewing, interaction, and storing data?","text":"<p>We are developing a comprehensive GUI based on ROS that serves multiple functions:</p> <ol> <li> <p>Real-time Data Visualization: </p> </li> <li> <p>Sensor data dashboards displaying IMU, barometer, GPS, camera, sonar, and LiDAR outputs.</p> </li> <li>Interactive 3D map showing current position, trajectory, and detected obstacles.</li> <li>System status indicators including battery levels, motor outputs, and connection quality.</li> </ol> <p></p> <ol> <li> <p>Control Interface:</p> </li> <li> <p>Mode selection panel (manual/autonomous).</p> </li> <li>Manual control input visualization.</li> <li>Goal setting and waypoint management tools.</li> <li>Parameter adjustment sliders for tuning control algorithms.</li> </ol> <p></p> <ol> <li> <p>Data Logging and Analysis:</p> </li> <li> <p>Comprehensive time-stamped data recording of all sensor inputs.</p> </li> <li>Performance metrics tracking, including stability measures and navigation accuracy.</li> <li>Export functionality for post-mission analysis.</li> <li>Replay capabilities for reviewing flight data.</li> </ol> <p>This integrated interface will not only enable immediate interaction during operation but also support detailed post-mission analysis and system refinement.</p>"},{"location":"#34-control-and-autonomy","title":"3.4 Control and Autonomy","text":""},{"location":"#how-do-you-plan-on-connecting-feedback-from-sensors-to-your-controller-in-higher-level-decision-making-processes-or-both","title":"How do you plan on connecting feedback from sensors to your controller, in higher-level decision-making processes, or both?","text":"<p>The system implements a multi-tiered approach to connect sensor feedback across both low-level control and high-level decision-making processes:</p> <p>Low-Level Control Integration:</p> <ul> <li>Stabilization Loop: IMU and barometer data feed directly into PID controllers that maintain attitude and altitude stability at 100Hz.</li> <li>Motor Control: Sensor-derived error signals adjust motor outputs to compensate for environmental disturbances.</li> <li>Sensor Fusion: Extended Kalman Filter combines IMU, barometer, and GPS data to provide accurate state estimation for the control system.</li> </ul> <p>High-Level Decision Making:</p> <ul> <li>Environment Mapping: LiDAR and camera data construct a dynamic 3D representation of the environment.</li> <li>Path Planning: A* algorithm utilizes the environmental map to generate efficient trajectories around obstacles.</li> <li>Goal Recognition: Computer vision algorithms process camera feeds to identify navigation targets.</li> <li>Mode Selection: Sensor data informs autonomous decisions about when to switch between different control behaviors.</li> </ul> <p>Cross-Level Integration:</p> <ul> <li>Adaptive Parameter Tuning: High-level processes adjust low-level control parameters based on environmental conditions.</li> <li>Hierarchical State Machine: Coordinates transitions between operational modes based on sensor-detected events.</li> <li>Performance Monitoring: Continuous evaluation of control effectiveness informs strategic decision adjustments.</li> </ul> <p>This comprehensive approach ensures sensor data flows seamlessly between tactical (moment-to-moment) and strategic (goal-oriented) layers of the system, enabling both responsive control and intelligent decision-making.</p>"},{"location":"#35-preparation-needs","title":"3.5 Preparation Needs","text":""},{"location":"#what-do-you-need-to-know-to-be-successful","title":"What do you need to know to be successful?","text":"<p>To achieve complete autonomy of the BLIMP, the team needs a comprehensive understanding of several interrelated technical areas:</p> <ol> <li>Sensor Fusion Algorithms: Advanced techniques for integrating heterogeneous sensor data into coherent state estimates.</li> <li>Aerodynamics of Lighter-than-Air Vehicles: Understanding the unique physics governing blimp movement and stability.</li> <li>Trajectory Planning for Non-Holonomic Systems: Specialized path planning considering the BLIMP's movement constraints.</li> <li>ROS2 Architecture: Deep knowledge of the communication patterns, service structures, and component interactions.</li> <li>Computer Vision for Navigation: Algorithms for visual odometry, feature detection, and goal recognition.</li> <li>Control Theory: Mathematical foundations for developing stable control systems in under-actuated platforms.</li> <li>Power Management: Strategies for maximizing flight time through efficient use of limited onboard energy.</li> </ol>"},{"location":"#which-of-those-topics-do-you-need-covered-in-class","title":"Which of those topics do you need covered in class?","text":"<p>For optimal project success, the team would benefit from classroom coverage of:</p> <ol> <li>Advanced PID Tuning Strategies: Practical methods for optimizing control parameters in multi-input, multi-output systems.</li> <li>Model Predictive Control (MPC): Theoretical foundations and implementation techniques in ROS for predictive navigation.</li> <li>ROS2 GUI Development: Hands-on instruction for creating intuitive interfaces using RQT or other frameworks.</li> <li>UAV Control Dynamics: Mathematical models specific to lighter-than-air vehicles.</li> <li>Sensor Fusion Techniques: Practical implementation of Extended Kalman Filters for state estimation.</li> <li>Remote System Administration: Efficient use of SSH and remote desktop tools for Raspberry Pi management.</li> <li>Real-time Performance Optimization: Strategies for managing computational resources on embedded platforms.</li> </ol> <p>These topics would provide the technical foundation required to successfully integrate sensor data and advanced control algorithms into a cohesive autonomous system.</p>"},{"location":"#36-final-demonstration","title":"3.6 Final Demonstration","text":""},{"location":"#please-describe-how-you-will-demonstrate-your-work-in-class","title":"Please describe how you will demonstrate your work in class","text":"<p>The final demonstration will showcase the BLIMP's autonomous capabilities in a comprehensive flight test. The system will:</p> <ol> <li>Take off and achieve stable hovering using barometer and IMU feedback.</li> <li>Identify visual targets placed throughout the demonstration area.</li> <li>Plan and execute an efficient trajectory toward these targets while maintaining altitude.</li> <li>Detect and navigate around obstacles in its path using LiDAR and sonar data.</li> <li>Demonstrate seamless switching between autonomous navigation and manual control.</li> <li>Display real-time sensor data, decision-making processes, and system status via the GUI.</li> </ol> <p>Throughout the demonstration, the team will explain the sensor fusion algorithms, control strategies, and decision-making processes occurring in real-time, highlighting how the different components work together to achieve autonomous operation.</p>"},{"location":"#what-resources-will-you-need","title":"What resources will you need?","text":"<p>Facility Requirements:</p> <ul> <li>Large indoor space (such as TECH 162) with at least 10m \u00d7 10m \u00d7 5m clearance.</li> <li>Power outlets for equipment charging and operation.</li> <li>Adequate lighting for camera-based navigation.</li> </ul> <p>Equipment Needs:</p> <ul> <li>Laptop with ROS2 and the custom GUI for system monitoring and control.</li> <li>Remote SSH capabilities for system access and troubleshooting.</li> <li>Joystick for manual control demonstrations and emergency override.</li> <li>Fully assembled BLIMP with all sensors integrated and calibrated.</li> <li>Visual markers and obstacles for navigation challenges.</li> <li>Backup batteries and emergency repair kit.</li> </ul>"},{"location":"#conditions-change-in-any-environment-how-will-your-robot-handle-variability-in-its-environment","title":"Conditions change in any environment. How will your robot handle variability in its environment?","text":"<p>The system is designed with adaptability as a core principle to handle environmental variability:</p> <p>Atmospheric Changes:</p> <ul> <li>Barometric pressure fluctuations will be compensated through dynamic altitude calculation adjustments.</li> <li>Temperature variations affecting sensor readings will be automatically calibrated through periodic reference checks.</li> </ul> <p>Lighting Conditions:</p> <ul> <li>The camera vision system employs adaptive exposure and contrast enhancement algorithms.</li> <li>Multiple visual recognition approaches (feature-based and neural network) provide redundancy across lighting conditions.</li> </ul> <p>Dynamic Obstacles:</p> <ul> <li>Real-time LiDAR scanning continuously updates the environmental map.</li> <li>Obstacle avoidance algorithms recalculate trajectories at 5Hz to respond to moving objects.</li> <li>Sonar provides redundant close-range detection for last-moment collision avoidance.</li> </ul> <p>Wind and Air Currents:</p> <ul> <li>IMU feedback detects unexpected orientation changes caused by air currents.</li> <li>PID controllers dynamically adjust motor outputs to compensate for external forces.</li> <li>Wind estimation algorithms adapt control parameters based on observed disturbance patterns.</li> </ul> <p>This multi-layered approach ensures the BLIMP remains stable and responsive even as environmental conditions change during operation.</p>"},{"location":"#testing-evaluation-plan-please-describe-how-you-can-verify-your-system-is-working-as-expected","title":"Testing &amp; Evaluation plan: Please describe how you can verify your system is working as expected","text":"<p>The testing and evaluation strategy follows a rigorous multi-stage approach:</p> <p>1. Unit Testing:</p> <ul> <li>Sensor Validation: Each sensor undergoes individual calibration and accuracy verification against known reference values.</li> <li>Control Module Testing: Isolated testing of individual control algorithms with simulated inputs.</li> <li>Component Integration: Verification of communication between adjacent system components.</li> </ul> <p>2. System Integration Testing:</p> <ul> <li>Sensor Fusion Accuracy: Comparison of fused state estimates against ground truth measurements.</li> <li>Control Loop Stability: Evaluation of system response to controlled disturbances.</li> <li>Resource Utilization: Monitoring of CPU, memory, and network usage during operation.</li> </ul> <p>3. Functional Testing:</p> <ul> <li>Hovering Stability: Quantitative measurement of position and attitude variance during stationary flight.</li> <li>Navigation Accuracy: Evaluation of path-following precision against predetermined routes.</li> <li>Obstacle Avoidance: Success rate in detecting and navigating around obstacles of varying sizes and positions.</li> <li>Goal Recognition: Accuracy and response time in identifying and moving toward visual targets.</li> </ul> <p>4. Performance Metrics:</p> <ul> <li>Position accuracy: Target &lt; 30cm deviation from planned path</li> <li>Altitude stability: \u00b110cm during stable hover</li> <li>Obstacle detection: 100% detection rate for objects &gt;20cm at distances up to 3m</li> <li>Battery efficiency: &gt;15 minutes of autonomous operation</li> <li>Control latency: &lt;50ms from sensor input to actuator response</li> </ul> <p>These comprehensive testing protocols will verify both individual component functionality and overall system performance against the design specifications.</p>"},{"location":"#37-impact-of-the-work","title":"3.7 Impact of the Work","text":""},{"location":"#provide-a-short-passage-about-the-impact-of-this-work-how-it-will-drive-you-to-learn-new-material-how-it-could-contribute-to-course-development-what-topics-it-will-encourage-learning-of-etc-and-any-other-important-points-not-addressed-above","title":"Provide a short passage about the impact of this work, how it will drive you to learn new material, how it could contribute to course development, what topics it will encourage learning of, etc., and any other important points not addressed above","text":"<p>This project represents a significant opportunity for both technical advancement and educational development:</p> <p>Technical Skill Development:</p> <p>This work will significantly enhance the practical skills by providing hands-on experience with ROS2, a cornerstone platform for modern robotics applications. The team members will gain invaluable expertise in autonomous navigation, trajectory planning, and real-time sensor fusion for accurate localization. The project's interdisciplinary nature will drive us to bridge the gap between theoretical knowledge and practical implementation, developing skills that are directly transferable to industry applications.</p> <p>Course Development Contributions:</p> <p>The AeroFusion platform will serve as a replicable template for future autonomous blimp initiatives in the course. The documented development process, including challenges and solutions, will provide a comprehensive case study for sensor-based control systems in unconventional UAVs. The modular nature of the design allows for incremental complexity in future course offerings, where students could focus on improving specific subsystems while maintaining overall functionality.</p> <p>Expanded Learning Horizons:</p> <p>This project will naturally drive us to explore advanced material beyond the standard curriculum, including: - Adaptive control strategies for under-actuated systems. - Machine learning approaches to environmental perception. - Energy-efficient computing for embedded systems. - Human-robot interaction design principles. - Robust engineering practices for field-deployable systems.</p> <p>Long-Term Impact: Beyond immediate educational benefits, this work lays the foundation for further research into lighter-than-air autonomous vehicles for applications including environmental monitoring, infrastructure inspection, and emergency response. The sensor fusion techniques we develop may inform approaches to similar challenges in other robotic domains, creating knowledge transfer opportunities across the field.</p>"},{"location":"#38-advising","title":"3.8 Advising","text":""},{"location":"#advisor","title":"Advisor:","text":"<p>Dr. Xi Yu (Faculty Advisor)</p> <p>Lab Space: TECH 189</p>"},{"location":"#required-resources","title":"Required Resources:","text":"<ul> <li>Guidance on trajectory planning algorithms.</li> <li>Access to high-precision GPS modules.</li> <li>Assistance in debugging sensor fusion issues.</li> </ul> <p>The team anticipates learning more about multi-sensor fusion algorithms, adaptive control strategies for dynamic environments, and real-time embedded system debugging.</p>"},{"location":"#4-weekly-milestones-weeks-7-16","title":"4. Weekly Milestones (Weeks 7-16)","text":""},{"location":"#updated-weekly-milestones-weeks-716","title":"\u2705 Updated Weekly Milestones (Weeks 7\u201316)","text":"Week Hardware Software Testing Status 7 Set up core hardware components Create basic user interface Test basic sensors (IMU, GPS) \u2705 8 Test power systems and batteries Develop interface mockups Install and test distance sensors \u2705 9 Test motors and speed controllers Connect interface to ROS Begin data collection from sensors \u2705 10 Extend hardware setup, wiring Add real-time sensor displays Calibrate initial sensors \u2705 11 Complete system integration Set up data storage + logging Begin combining sensor inputs \u2705 12 Start motor tuning + prop checks Add map view to interface Initial static testing of camera \u2705 13 Prep for flight &amp; safety checks Finalize ROS visualization setup Validate sensor fusion \ud83d\udd04 14 Conduct initial flight tests \u2708\ufe0f Basic control tuning via interface Monitor live telemetry, adjust logs \ud83d\udd04 15 Refine hardware from flight data UI polish &amp; final features Autonomous flight path tuning \ud83d\udd52 16 Final fixes and mounting upgrades Bugfix and code cleanup Final demo rehearsal &amp; testing \ud83d\udd52 <p>Legend: </p> <ul> <li>\u2705 = Completed  </li> <li>\ud83d\udd04 = In Progress  </li> <li>\ud83d\udd52 = Pending</li> </ul>"},{"location":"gantt/","title":"Gantt Chart","text":"<pre><code>%%{init: {'themeVariables': {'opacity': '1'}}}%%\ngantt\n    title Simplified Project Timeline (March - May)\n    dateFormat  YYYY-MM-DD\n\n    section Hardware &amp; Sensors\n    Hardware setup &amp; testing       :done,   2025-03-04, 21d\n    System wiring &amp; integration    :done,   2025-03-25, 7d\n    Flight tests &amp; optimization    :active, 2025-04-01, 14d\n    Final refinements &amp; demo prep  :active, 2025-04-17, 14d\n\n    section Software &amp; Interface\n    GUI design &amp; mockups           :done,   2025-03-24, 14d\n    ROS integration &amp; visualization:done,   2025-03-10, 20d\n    UI development &amp; testing       :active, 2025-04-08, 14d\n    Documentation &amp; finalization   :planned, 2025-04-22, 10d\n\n    section Controls &amp; Autonomy\n    Manual control &amp; tuning        :done,   2025-03-04, 14d\n    Stability &amp; trajectory planning:done,   2025-03-18, 14d\n    Autonomy &amp; path following      :active, 2025-04-08, 14d\n    Final optimization &amp; demo      :planned, 2025-04-24, 10d</code></pre>"},{"location":"second-page/","title":"Current Project Status","text":""},{"location":"second-page/#team-assignment-4","title":"Team Assignment 4","text":"<p>Since the last milestone, the team has made substantial progress in developing both the sensing and autonomy stack. the primary focus is building a system that can identify and track an olive-colored balloon in real time, navigate toward it using differential drive, and determine success based on proximity to the center of the camera frame.</p> <p>This work integrates multiple sensing modalities: camera, IMU, and barometer alongside autonomous behavior through ROS2 nodes and coordinated control logic.</p>"},{"location":"second-page/#sensor-integration-and-filtering","title":"Sensor Integration and Filtering","text":"<p>The team is currently using three primary sensors:</p> <ul> <li>Camera (USB webcam) for image acquisition and target detection</li> <li>ICM20948 IMU for measuring orientation and motion</li> <li>Barometer for height estimation and stabilization</li> </ul> <p>The IMU and barometer data undergo filtering and fusion, providing stable orientation and altitude feedback. The camera provides vision-based input used primarily for high-level object detection and tracking.</p>"},{"location":"second-page/#active-ros-topics-place-holder","title":"Active ROS Topics (place holder)","text":"<pre><code>/image_raw                # Camera feed \n/blimp/detected_position  # YOLO-based balloon detection \n/imu/data_raw             # IMU (gyro, accel, fused orientation) \n/imu/mag                  # Magnetometer readings \n/barometer/pressure       # Altitude estimation\n</code></pre> <p>The fusion of IMU and barometer data allows for stable low-level control, while the camera provides the primary input for high-level decision-making.</p>"},{"location":"second-page/#object-detection-and-tracking","title":"## Object Detection and Tracking","text":"<p>The team trained a custom YOLOv5s model to detect olive-colored balloons, which are the objects of interest.</p> <p>Training details: - Platform: Roboflow + local training - Dataset: Collected in variable lighting/backgrounds - mAP@0.5: ~96% - Inference time: ~20ms (GPU), &lt;100ms (CPU)</p> <p>Upon detection, the team calculate the centroid of the balloon and convert it to relative pixel coordinates with the frame\u2019s center as origin. This allows quadrant classification (<code>Q1\u2013Q4</code>) and distance estimation.  </p> <p>The team used the object's position relative to the center to command differential drive actions. As the object nears the center, the blimp slows down and halts when it reaches a defined \"success zone.\"</p>"},{"location":"second-page/#object-tracking-and-success-criteria","title":"Object Tracking and Success Criteria","text":"<p>The object tracking strategy is based on the distance between the detected object\u2019s centroid and the center of the frame. The logic is as follows:</p> <ol> <li>YOLO detects the object and computes its (x, y) position.</li> <li>The team measured the offset from the image center.</li> <li>This offset is used to determine the movement direction via differential drive.</li> <li>When the object enters a predefined central \"success zone\", the task is marked complete and the blimp halts.</li> </ol> <p>This behavior mimics a form of visual servoing, using object position rather than absolute coordinates to navigate.</p>"},{"location":"second-page/#low-level-and-high-level-autonomy","title":"Low-Level and High-Level Autonomy","text":""},{"location":"second-page/#low-level-autonomy","title":"Low-Level Autonomy","text":"<p>Low-level autonomy includes all real-time control and stabilization functions:</p> <ul> <li>IMU + Barometer Fusion: Estimating attitude (roll, pitch, yaw) and height.</li> <li>Sensor Filtering:</li> <li>Low-pass filters on accelerometer data to reduce noise</li> <li>Complementary filters for fusing orientation data</li> <li>PID Control (planned): For motor and throttle regulation.</li> <li>Motor Feedback (encoders): Used for velocity and position tracking.</li> </ul>"},{"location":"second-page/#high-level-autonomy","title":"High-Level Autonomy","text":"<p>High-level autonomy focuses on decision-making and planning:</p> <ul> <li>Object detection with YOLOv5: Identifying the balloon in frame</li> <li>Quadrant classification: Mapping position to Q1\u2013Q4 based on pixel coordinates</li> <li>Movement control: Commands based on detected quadrant</li> <li>Goal logic:  </li> <li>If object is off-center \u2192 adjust orientation and move</li> <li>If object is centered \u2192 stop motors and flag task as completed</li> <li>Fallback logic (planned): Handle scenarios where the object leaves the frame</li> </ul>"},{"location":"second-page/#conditioning-and-filtering-of-sensor-data","title":"Conditioning and Filtering of Sensor Data","text":"<p>Effective autonomy depends on reliable sensor input. Here\u2019s how the team will manage each:</p>"},{"location":"second-page/#camera-vision-input","title":"Camera (Vision Input)","text":"<ul> <li>Contrast Enhancement: Histogram equalization and HSV color space for better robustness under changing lighting.</li> <li>Filtering: Median filtering and Gaussian blurring to suppress noise.</li> <li>Region of Interest (ROI): Focus processing on central region to improve performance.</li> </ul>"},{"location":"second-page/#imu","title":"IMU","text":"<ul> <li>Sensor Fusion: Combining gyro, accel, and mag data using complementary filters.</li> <li>Filtering: Low-pass filtering for accel and magnetometer data; high-pass to correct gyro drift.</li> </ul>"},{"location":"second-page/#barometer","title":"Barometer","text":"<ul> <li>Smoothing: Simple moving average to eliminate spikes.</li> <li>Correlation: Used alongside IMU for stable vertical state estimation.</li> </ul>"},{"location":"second-page/#motors","title":"Motors","text":"<ul> <li>PID Controllers (planned): For consistent and smooth motion.</li> <li>Encoder Feedback: To adjust velocity in real time.</li> <li>Noise Filtering: Reduce jitter in encoder readings via smoothing.</li> </ul>"},{"location":"second-page/#decision-making-overview","title":"Decision-Making Overview","text":"<p>Sensor data feeds into both short-term (control) and long-term (behavioral) decisions:</p>"},{"location":"second-page/#low-level-decisions","title":"Low-Level Decisions","text":"<ul> <li>Set PWM/motor speeds based on position errors</li> <li>Adjust throttle for altitude using barometer</li> <li>Stabilize yaw using IMU orientation</li> <li>Apply real-time filters for smoother control</li> </ul>"},{"location":"second-page/#high-level-decisions","title":"High-Level Decisions","text":"<ul> <li>Use YOLO to detect and localize the balloon</li> <li>Estimate relative direction and command movement</li> <li>Track proximity to frame center and decide when to stop</li> <li>Manage behavioral states: searching \u2192 tracking \u2192 stop</li> </ul>"},{"location":"second-page/#updated-sensor-flowchart","title":"Updated Sensor Flowchart","text":"<pre><code>graph TD\n    Camera --&gt;|/image_raw| YOLOv5_Detector\n    YOLOv5_Detector --&gt;|/blimp/detected_position| QuadrantLogic\n    IMU --&gt;|/imu/data_raw| Stabilizer\n    Barometer --&gt;|/barometer/pressure| Stabilizer\n    QuadrantLogic --&gt; MotionPlanner\n    Stabilizer --&gt; MotionPlanner\n    MotionPlanner --&gt; Actuators</code></pre>"},{"location":"second-page/#need-rqt-graph","title":"Need: rqt graph.....","text":""},{"location":"second-page/#summary","title":"Summary","text":"<p>The system can now: </p> <ul> <li>Detect and track a balloon in real time</li> <li>Move toward the object using differential drive</li> <li>Halt autonomously when a success zone is reached</li> <li>Publish and use filtered sensor data from IMU and barometer</li> </ul> <p>Next Goals:</p> <ul> <li>Integrate PID control for smoother navigation</li> <li>Add fallback logic if the object is lost</li> <li>Tune the success detection thresholds</li> <li>Record and analyze system performance using rosbag</li> </ul>"},{"location":"static/node_modules/mathjax/","title":"MathJax","text":""},{"location":"static/node_modules/mathjax/#beautiful-math-in-all-browsers","title":"Beautiful math in all browsers","text":"<p>MathJax is an open-source JavaScript display engine for LaTeX, MathML, and AsciiMath notation that works in all modern browsers.  It was designed with the goal of consolidating the recent advances in web technologies into a single, definitive, math-on-the-web platform supporting the major browsers and operating systems.  It requires no setup on the part of the user (no plugins to download or software to install), so the page author can write web documents that include mathematics and be confident that users will be able to view it naturally and easily.  Simply include MathJax and some mathematics in a web page, and MathJax does the rest.</p> <p>Some of the main features of MathJax include:</p> <ul> <li> <p>High-quality display of LaTeX, MathML, and AsciiMath notation in HTML pages</p> </li> <li> <p>Supported in most browsers with no plug-ins, extra fonts, or special   setup for the reader</p> </li> <li> <p>Easy for authors, flexible for publishers, extensible for developers</p> </li> <li> <p>Supports math accessibility, cut-and-paste interoperability, and other   advanced functionality</p> </li> <li> <p>Powerful API for integration with other web applications</p> </li> </ul> <p>See http://www.mathjax.org/ for additional details about MathJax, and https://docs.mathjax.org for the MathJax documentation.</p>"},{"location":"static/node_modules/mathjax/#mathjax-components","title":"MathJax Components","text":"<p>MathJax version 3 uses files called components that contain the various MathJax modules that you can include in your web pages or access on a server through NodeJS.  Some components combine all the pieces you need to run MathJax with one or more input formats and a particular output format, while other components are pieces that can be loaded on demand when needed, or by a configuration that specifies the pieces you want to combine in a custom way.  For usage instructions, see the MathJax documentation.</p> <p>Components provide a convenient packaging of MathJax's modules, but it is possible for you to form your own custom components, or to use MathJax's modules directly in a node application on a server.  There are web examples showing how to use MathJax in web pages and how to build your own components, and node examples illustrating how to use components in node applications or call MathJax modules directly.</p>"},{"location":"static/node_modules/mathjax/#whats-in-this-repository","title":"What's in this Repository","text":"<p>This repository contains only the component files for MathJax, not the source code for MathJax (which are available in a separate MathJax source repository).  These component files are the ones served by the CDNs that offer MathJax to the web.  In version 2, the files used on the web were also the source files for MathJax, but in version 3, the source files are no longer on the CDN, as they are not what are run in the browser.</p> <p>The components are stored in the <code>es5</code> directory, and are in ES5 format for the widest possible compatibility.  In the future, we may make an <code>es6</code> directory containing ES6 versions of the components.</p>"},{"location":"static/node_modules/mathjax/#installation-and-use","title":"Installation and Use","text":""},{"location":"static/node_modules/mathjax/#using-mathjax-components-from-a-cdn-on-the-web","title":"Using MathJax components from a CDN on the web","text":"<p>If you are loading MathJax from a CDN into a web page, there is no need to install anything.  Simply use a <code>script</code> tag that loads MathJax from the CDN.  E.g.,</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See the MathJax documentation, the MathJax Web Demos, and the MathJax Component Repository for more information.</p>"},{"location":"static/node_modules/mathjax/#hosting-your-own-copy-of-the-mathjax-components","title":"Hosting your own copy of the MathJax Components","text":"<p>If you want to host MathJax from your own server, you can do so by installing the <code>mathjax</code> package using <code>npm</code> and moving the <code>es5</code> directory to an appropriate location on your server:</p> <pre><code>npm install mathjax@3\nmv node_modules/mathjax/es5 &lt;path-to-server-location&gt;/mathjax\n</code></pre> <p>Note that we are still making updates to version 2, so include <code>@3</code> when you install, since the latest chronological version may not be version 3.</p> <p>Alternatively, you can get the files via GitHub:</p> <pre><code>git clone https://github.com/mathjax/MathJax.git mj-tmp\nmv mj-tmp/es5 &lt;path-to-server-location&gt;/mathjax\nrm -rf mj-tmp\n</code></pre> <p>Then (in either case) you can use a script tag like the following:</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"&lt;url-to-your-site&gt;/mathjax/tex-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>where <code>&lt;url-to-your-site&gt;</code> is replaced by the URL to the location where you moved the MathJax files above.</p> <p>See the documentation for details.</p>"},{"location":"static/node_modules/mathjax/#using-mathjax-components-in-a-node-application","title":"Using MathJax components in a node application","text":"<p>To use MathJax components in a node application, install the <code>mathjax</code> package:</p> <pre><code>npm install mathjax@3\n</code></pre> <p>(we are still making updates to version 2, so you should include <code>@3</code> since the latest chronological version may not be version 3).</p> <p>Then require <code>mathjax</code> within your application:</p> <pre><code>require('mathjax').init({ ... }).then((MathJax) =&gt; { ... });\n</code></pre> <p>where the first <code>{ ... }</code> is a MathJax configuration, and the second <code>{ ... }</code> is the code to run after MathJax has been loaded.  E.g.</p> <pre><code>require('mathjax').init({\nloader: {load: ['input/tex', 'output/svg']}\n}).then((MathJax) =&gt; {\nconst svg = MathJax.tex2svg('\\\\frac{1}{x^2-1}', {display: true});\nconsole.log(MathJax.startup.adaptor.outerHTML(svg));\n}).catch((err) =&gt; console.log(err.message));\n</code></pre> <p>Note: this technique is for node-based application only, not for browser applications.  This method sets up an alternative DOM implementation, which you don't need in the browser, and tells MathJax to use node's <code>require()</code> command to load external modules.  This setup will not work properly in the browser, even if you webpack it or bundle it in other ways.</p> <p>See the documentation and the MathJax Node Repository for more details.</p>"},{"location":"static/node_modules/mathjax/#reducing-the-size-of-the-components-directory","title":"Reducing the Size of the Components Directory","text":"<p>Since the <code>es5</code> directory contains all the component files, so if you are only planning one use one configuration, you can reduce the size of the MathJax directory by removing unused components. For example, if you are using the <code>tex-chtml.js</code> component, then you can remove the <code>tex-mml-chtml.js</code>, <code>tex-svg.js</code>, <code>tex-mml-svg.js</code>, <code>tex-chtml-full.js</code>, and <code>tex-svg-full.js</code> configurations, which will save considerable space.  Indeed, you should be able to remove everything other than <code>tex-chtml.js</code>, and the <code>input/tex/extensions</code>, <code>output/chtml/fonts/woff-v2</code>, <code>adaptors</code>, <code>a11y</code>, and <code>sre</code> directories.  If you are using the results only on the web, you can remove <code>adaptors</code> as well.</p> <p>If you are not using A11Y support (e.g., speech generation, or semantic enrichment), then you can remove <code>a11y</code> and <code>sre</code> as well (though in this case you may need to disable the assistive tools in the MathJax contextual menu in order to avoid MathJax trying to load them when they aren't there).</p> <p>If you are using SVG rather than CommonHTML output (e.g., <code>tex-svg.js</code> rather than <code>tex-chtml.js</code>), you can remove the <code>output/chtml/fonts/woff-v2</code> directory.  If you are using MathML input rather than TeX (e.g., <code>mml-chtml.js</code> rather than <code>tex-chtml.js</code>), then you can remove <code>input/tex/extensions</code> as well.</p>"},{"location":"static/node_modules/mathjax/#the-component-files-and-pull-requests","title":"The Component Files and Pull Requests","text":"<p>The <code>es5</code> directory is generated automatically from the contents of the MathJax source repository.  You can rebuild the components using the command</p> <pre><code>npm run make-es5 --silent\n</code></pre> <p>Note that since the contents of this repository are generated automatically, you should not submit pull requests that modify the contents of the <code>es5</code> directory.  If you wish to submit a modification to MathJax, you should make a pull request in the MathJax source repository.</p>"},{"location":"static/node_modules/mathjax/#mathjax-community","title":"MathJax Community","text":"<p>The main MathJax website is http://www.mathjax.org, and it includes announcements and other important information.  A MathJax user forum for asking questions and getting assistance is hosted at Google, and the MathJax bug tracker is hosted at GitHub.</p> <p>Before reporting a bug, please check that it has not already been reported.  Also, please use the bug tracker (rather than the help forum) for reporting bugs, and use the user's forum (rather than the bug tracker) for questions about how to use MathJax.</p>"},{"location":"static/node_modules/mathjax/#mathjax-resources","title":"MathJax Resources","text":"<ul> <li>MathJax Documentation</li> <li>MathJax Components</li> <li>MathJax Source Code</li> <li>MathJax Web Examples</li> <li>MathJax Node Examples</li> <li>MathJax Bug Tracker</li> <li>MathJax Users' Group</li> </ul>"}]}