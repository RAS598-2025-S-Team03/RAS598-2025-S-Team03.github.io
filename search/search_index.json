{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Team Number: 03 </p> <p>Team Members: - Nihar Masurkar - Prajjwal Dutta - Sai Srinivas Tatwik Meesala</p> <p>Semester and Year: Spring 2025  </p> <p>University, Class, and Professor: </p> <ul> <li>University: Arizona State University  </li> <li>Class: RAS 598: Experimentation and Deployment of Robotic Systems</li> <li>Professor: Dr. Daniel Aukes</li> </ul>"},{"location":"#1-introduction","title":"1. Introduction","text":"<p>The project aims to develop an integrated, sensor-driven framework that enables a biologically inspired, lighter-than-air, Instructional, Mechatronics Program (BLIMP) UAV to operate autonomously and identify and track objects of interest in dynamic and uncertain environments.  </p> <p>The central research question is: \"How effectively can sensor data from various sensors (such as IMU, Barometer and Camera) be fused to enhance the control systems and tracking capabilities of a hybrid robotic blimp system in dynamic environments?\" </p> <p> Figure 1: CAD Rendering of Biologically-inspired, Lighter-than-air, Instructional, Mechatronics Program (BLIMP) UAV </p> <p>The system aforementioned is an unmanned aerial system (UAS) that is used in the Defend the Republic competition. The UAS vehicles used resemble conventional airship profiles with similarities in maneuverability and control. While the problem of trajectory tracking is well researched, difficulties arise with lighter-than-air vehicles (LTAVs) and similar vehicles, as highly nonlinear behavior greatly defines the behavior of the system. Likewise, environmental disturbances can also influence system response. Prior research in the area has dealt with implementing PID and force-predictive controller strategies [1]. In terms of LTA vehicles specifically, the implementation of Linear Quadratic Regulator (LQR) control was explored utilizing conventional airship dynamics to represent the system behavior [6]. With the growing interest in autonomous technology, the use of smaller unmanned vehicles for research, academia, and military applications is an ongoing research topic. Consequently, effective and safe control strategies for such vehicles are of particular importance.  </p> <p>For this project, and an invested interest in the coordination of unmanned underwater vehicles (UUVs), it was chosen to implement conventional UUV dynamics using an LTAV as the test bed vehicle. Using various methods, model parameters were identified, and the similarities in the model were proven. Different control strategies were investigated, including those aforementioned; however, it was observed that such control strategies were not sufficient for the system, due to high nonlinearities and the need to optimize trajectory tracking controls online to follow multiple trajectories. Consequently, a model predictive controller (MPC) was chosen for the system due to its effectiveness in producing online optimal control inputs and its efficiency at handling highly nonlinear systems.  </p>"},{"location":"#updated-project-goals","title":"Updated Project Goals","text":"<p>The project's original goal was to enable an autonomous blimp platform capable of detecting, tracking, and approaching an olive-colored balloon using computer vision and onboard sensors. Since the last update, we have refined our objectives to focus more explicitly on:</p> <ul> <li>Reliable balloon detection using a trained YOLOv5 model.</li> <li>Quadrant-based navigation logic based on object centroid position.</li> <li>Sensor fusion using the IMU and barometer for stabilized flight control.</li> <li>Differential drive-based motion to minimize hardware complexity.</li> <li>Real-time object tracking and task success based on position proximity to the image center.</li> </ul> <p>These refinements helped narrow our scope and allowed us to focus efforts on perception-control integration while still meeting the autonomy requirements.</p>"},{"location":"#2-methods","title":"2. Methods","text":""},{"location":"#21-modeling","title":"2.1. Modeling","text":"<p>To properly implement the control system, a dynamic model of the blimp had to be developed. The following equations (1) and (2) can be used to represent the dynamics of the LTA. The dynamic model includes both the kinematics (1) and the kinetics of the system (2). The kinematics of the system deals with how the object moves through space, while the kinetics of the system deals with how forces affect the motion of the vehicle. Before jumping to specific dynamic equations of the LTA, it is important to first define the reference frames and symbolic meaning of the variables used throughout the report. Two reference frames were used in developing the dynamic model: the inertial (Earth) reference frame and the fixed body frame. Figure 1 illustrates the reference frames that were considered.  </p> <p> </p> <p>The inertial reference frame is \u201cfixed\u201d concerning the Earth. In most applications, it is assumed to be inertial, meaning the reference frame is subject to no acceleration. The body reference frame on the other hand is defined based on non-collinear points along the airship as illustrated above, with the x-axis pointing in the direction of forward motion, y-axis pointed towards the starboard (right) side of the airship, and the z-axis perpendicular to the plane produced by pointing downward through the hull. The 6 DOF model is used to represent the system.  </p> <p>In the kinematic and kinetic equations above, subscript E denotes the inertial frame while subscript b denotes the body frame. Due to the complexity of the model, matrix notation was adopted for ease of calculation and representation. In the model \ud835\udc65\ud835\udc4f,\ud835\udc65\u0307\ud835\udc4f,\ud835\udc65\u0308\ud835\udc4f are [6x1] vectors representing the vehicle's position, velocity, and acceleration in the body frame, respectively, while \ud835\udc65\ud835\udc38 is a [6x1] vector representing the position of the vehicle in the Earth coordinate system. Vector notation is described as follows.  </p> <p> </p> <p>In the equations above, the first three vector elements represent linear positions, velocities, and accelerations, while the last three vector elements represent angular positions, velocities, and accelerations in their respective coordinate systems. Inertial properties of the vehicle are accounted for by the [6x6] mass matrix, M. Coriolis and centripetal effects are accounted for by the [6x6] matrix, C. Damping effects are accounted for by the [6x6] damping matrix, D, and restoring forces and moments are accounted for by the [1x6] vector, \ud835\udc54. The propulsive forces and moment acting on the vehicle are described by the [6x1] vector,\ud835\udf0f\ud835\udc4f. The aforementioned values will be described in detail in the succeeding sections.  </p>"},{"location":"#22-kinematic-equation","title":"2.2. Kinematic Equation","text":"<p>Euler angles can be used to transform both linear and angular velocities between the body frame and the inertial (NED) frame. The transformation for linear velocities from the body reference frame to the Earth reference frame is given by the following equation.  </p> <p> </p> <p>Also, if it is warranted to transform the system from an inertial frame to the body frame, the transpose of the above equation is adopted \ud835\udc3d\ud835\udc47(\u0398). Another important realization is that a pitch angle, \ud835\udf03, equivalent to 90\u00b0, produces a singularity as there is no unique solution for angles \ud835\udf11 and \ud835\udf13. In such a case quaternion can be used to represent the orientation; however, it is not expected for the system to pitch at an angle close to 90\u00b0, so the Euler angle representation was considered to be sufficient for the model.  </p>"},{"location":"#23-mass-matrix","title":"2.3. Mass Matrix","text":"<p>The mass matrix consists of the masses due to the rigid body and added or virtual masses. The added or virtual masses and inertia are usually not considered in conventional aircraft dynamics but can arise in the case of LTA vehicles and underwater vehicles due to the vehicle mass being the same order of magnitude as the displaced fluid. Therefore, the additional force resulting from the fluid acting on the structure must also be considered. The rigid body mass matrix is described in (14).  </p> <p> </p> <p>In the matrix above, symbolic variable notation is as follows. \ud835\udc5a is the mass of the vehicle. \ud835\udc3c\ud835\udc65, \ud835\udc3c\ud835\udc66, and \ud835\udc3c\ud835\udc67 are the moments of inertia about the \ud835\udc65\ud835\udc4f, \ud835\udc66\ud835\udc4f, and \ud835\udc67\ud835\udc4f axes in the body reference frame. \ud835\udc3c\ud835\udc65\ud835\udc66=\ud835\udc3c\ud835\udc66\ud835\udc65, \ud835\udc3c\ud835\udc65\ud835\udc67=\ud835\udc3c\ud835\udc67\ud835\udc65, and \ud835\udc3c\ud835\udc66\ud835\udc67=\ud835\udc3c\ud835\udc67\ud835\udc66 are the products of the moments of inertia, and \ud835\udc5f\ud835\udc54=[\ud835\udc65\ud835\udc54,\ud835\udc66\ud835\udc54,\ud835\udc67\ud835\udc54]\ud835\udc47 is the position of the COG (center of gravity) relative to the COB (center of buoyancy of the vehicle). Following this notation, a few assumptions can be made regarding the vehicle. At a base level, an assumption can be made that the vehicle is symmetric about all axes. Thus, the products of the moments of inertia can be simplified such that \ud835\udc3c\ud835\udc65\ud835\udc66=\ud835\udc3c\ud835\udc66\ud835\udc67=\ud835\udc3c\ud835\udc67\ud835\udc65=0. It can also be assumed that the position of the COG relative to the center of buoyancy is equivalent in both the x and y directions. Thus, \ud835\udc65\ud835\udc54=\ud835\udc66\ud835\udc54=0. However, due to the presence of the gondola, which holds a significant amount of weight about the vehicle, \ud835\udc67\ud835\udc54\u22600. This will reduce (14) to (15) below.  </p> <p> </p> <p> </p>"},{"location":"#24-coriolis-and-centripetal-matrix","title":"2.4. Coriolis and Centripetal Matrix","text":"<p>The Coriolis and centripetal matrix contains the dynamic terms associated with the linear and angular velocities in the body reference frame. The Coriolis matrix is derived directly from the rigid body and added mass matrix. The Coriolis force is an inertial force that acts on objects in motion that rotate within a frame of reference (body reference frame) to an inertial frame. Centripetal force accounts for objects moving along circular paths. Matrix (19) below was used to account for these phenomena in the system [7].  </p> <p> </p>"},{"location":"#25-damping-matrix","title":"2.5. Damping Matrix","text":"<p>The aerodynamic damping matrix is caused by friction effects, which are a function of the velocity of the vehicle. Two fundamental friction regions are considered: linear friction due to the laminar boundary layer and quadratic friction due to the turbulent boundary layer. This matrix also contains the effects of vortex shedding along the flow line. As indicated in [7], nondiagonal terms are relatively small and can be ignored. As such, a diagonal structure is developed for the damping matrix and can be approximated per (20).  </p> <p> </p> <p>With the assumption that the vehicle will be in an indoor environment and operating at low speeds, a laminar boundary layer can be considered. Thus, the quadrating damping terms can be ignored, leaving only the linear skin friction coefficient.  </p> <p> </p> <p>The coefficients presented above for the damping matrix can be estimated from wind-tunnel testing or system identification tools.  </p>"},{"location":"#26-restoring-forces-and-moments-vector","title":"2.6. Restoring Forces and Moments Vector","text":"<p>The restoring forces and moments term is a combination of gravity and buoyancy forces and moments. The buoyancy force is an aerostatic lift force acting on the vehicle and is derived from Archimedes' principle, which states that the buoyancy force is equivalent to the weight of the displaced fluid occupied by the volume of the vehicle. The two forces can be described as follows.  </p> <p> </p> <p>In the above formula, the volume, \ud835\udc49, of an ellipsoidal balloon can be described by \ud835\udc49=4/3\ud835\udf0b\ud835\udc4e\ud835\udc4f^2. Expressing these above terms in the inertial reference frame using Euler angle transformation yields (24).  </p> <p> </p> <p>As aforementioned, the center of buoyancy is assumed to be located at the origin of the body. Thus, \ud835\udc5f\ud835\udc4f=[\ud835\udc65\ud835\udc4f \ud835\udc66\ud835\udc4f \ud835\udc67\ud835\udc4f]\ud835\udc47=[0 0 0]\ud835\udc47, and it is assumed that the x and y center of gravity positions coincide with the origin of the vehicle; however, due to the weight of the gondola, the z-directional center of gravity does not. Thus, \ud835\udc5f\ud835\udc54=[\ud835\udc65\ud835\udc54 \ud835\udc66\ud835\udc54 \ud835\udc67\ud835\udc54], \ud835\udc47=[0 0 \ud835\udc67\ud835\udc54]\ud835\udc47. Therefore, the restoring forces and moments vector is reduced to the following.  </p> <p> </p>"},{"location":"#27-propulsion-forces-and-moments","title":"2.7. Propulsion Forces and Moments","text":"<p>The control input is a vector of forces and moments. The produced forces are dependent on the number of thrusters, the mounting position, and the orientation of the thrusters. The general notation for the propulsion forces and moments is described by (26).  </p> <p> </p> <p>In terms of control inputs, the propulsion forces and moments can be written per the following.  </p> <p> </p> <p>In the above equation, \ud835\udc47 is the thrust configuration matrix. In the case of the current vehicle setup, there are a total of four thrusters producing \ud835\udc47=[\ud835\udc471 \ud835\udc472 \ud835\udc473 \ud835\udc474]. The forces and moments produced by each thruster can be calculated per the following equation.  </p> <p>As noted in (27), the force is a function of the control input (PWM) signal and the thrust coefficient matrix, K, which describes how the PWM signals relate to the thrust force for a specific motor setup. These values can be experimentally calculated and will be described in a later section.  </p>"},{"location":"#28-control-system","title":"2.8. Control System","text":"<p>Control allocation allows the computation of the input signal, \ud835\udc62, to apply to the thrusters for the propulsive forces and moments to be produced.  </p> <p> </p> <p>It should be noted that the thruster allocation matrix is non-square due to there being a total of four thrusters and 6 DOF. Consequently, an alternative method to find the inverse is needed, such as the Moore-Penrose pseudo-inverse. Therefore, the control input vector, \ud835\udc62, can be determined by the following equation.  </p> <p> </p> <p>It should be noted that the MPC controller developed calculates an optimized control input to each motor using a different methodology than described above. However, if tuning the system via PID control, equation (30) can be used to calculate the control input based upon some error that is tuned by the control gains.  </p>"},{"location":"#29-linearization","title":"2.9. Linearization","text":"<p>For practical purposes, it is necessary to discretize and linearize the dynamic model. As previously noted, the vehicle dynamics are highly nonlinear. Therefore, to handle this issue a state space model was developed which provides a convenient platform for handling multiple-input-multiple-output (MIMO) systems. The state vector [12x1] and control input vector [4x1] were identified and are shown below.  </p> <p> </p> <p>At this point, the general form of the nonlinear state space model can be developed per the following equations.  </p> <p> </p> <p>For the proposed model, it is assumed that the output of the state space model, \ud835\udc66(\ud835\udc61), is equivalent to the current state of the system \ud835\udc65(\ud835\udc61). Expressing the kinetic equation in terms of the highest order state (acceleration) gives.  </p> <p> </p> <p>Solving this equation allows us to write the continuous-time state-space formulation as described in (36) and (37).  </p> <p> </p> <p>In order to find the state transition matrix, \ud835\udc34, and the input matrix, \ud835\udc35, the Jacobian matrix is needed to make a linear approximation of the nonlinear system. The Jacobian makes a linear approximation of the nonlinear function around a point of interest using a first order Taylor Series expansion. The general form of the Jacobian linearization matrix is presented below.  </p> <p> </p> <p>Thus, the state transition matrix, \ud835\udc34, and input matrix, \ud835\udc35, can be described by the Jacobian linearization such that.  </p> <p> </p>"},{"location":"#3-system-identification","title":"3. System Identification","text":"<p>The dynamic model of the vehicle described above requires the identification of the system parameters listed below.  </p> <p> </p> <p>There are various ways to go about calculating the unknown parameters in the system. For the purposes of the project SolidWorks CAD modeling, theoretical calculations, and experimental tests were performed to identify and validate the model.  </p>"},{"location":"#31-cad-model-measurements","title":"3.1. CAD Model Measurements","text":"<p>Due to the various vehicle prototypes being developed throughout the course of the semester, a CAD model was employed to find inertial and geometrical characteristics that could easily be updated to represent the current configuration of the vehicle.  </p> <p> </p> <p>The CAD model in Figure 3 provides a framework to reassess the geometrical and inertial parameters with different envelope sizes, motors, gondola and net configurations. Using this model the following parameters were identified.  </p> <p> </p> <p>Particularly interesting in the results are the rigid body mass matrix and center of gravity matrix. In the modeling section, it was noted that, due to symmetry, off-diagonal terms in the rigid body mass matrix were negligible, and that the weight of the gondola would affect only the center of gravity in the z-direction. This assumption is reflected in the results for the SolidWorks CAD model of the vehicle.  </p>"},{"location":"#32-theoretical-calculations","title":"3.2. Theoretical Calculations","text":"<p>To calculate the virtual/added mass factors, a theoretical method is utilized that uses the geometry of the vehicle and the kinetic energy of an ideal fluid volume around the moving vehicle. Virtual mass calculations are equivalent to the density of the surrounding fluid multiplied by a volume that depends on the geometric shape of the body. Lamb\u2019s k-factor method was utilized to calculate the added mass of the balloon. Whereas the ellipsoid equation in the body frame is represented by (41).  </p> <p> </p> <p>Applying symmetry and Lamba k-factors to the vehicle, the added mass factors can be calculated using Lamb\u2019s k-factor coefficients.  </p> <p> </p> <p>Applying the above equations, the added mass values can be found and are represented in Table 3 below.  </p> <p> </p> <p>Using the identified values from both the CAD model and the theoretical calculations, the mass, Coriolis, and centripetal matrices were accounted for. Thus, the only remaining parameters left to be identified are the thrust coefficient matrix and the damping matrix.  </p>"},{"location":"#33-experimental-calculations","title":"3.3. Experimental Calculations","text":"<p>The thrust coefficient matrix can be calculated directly from thrust stand data. In this test, a known PWM (pulse width modulation) signal is sent to the motor, which is attached to a stationary stand that measures the thrust force of the motor configuration. The current vehicle configuration uses a Park 180 motor for the left and right thrusters and a Park 250 motor for the up and down motors. A step test was performed for each motor in which the PWM signal was varied from 1100 to 1800 in increments of 50, and the resulting data is shown in Table 4 below.</p> <p> </p> <p>The data above was then plotted for each motor. As Figures 4 and 5 below illustrate, Thrust vs. PWM exhibits a near-linear relationship. Thus, a linear equation was fit to the data to be used in the thrust coefficient matrix.  </p> <p> </p> <p>Unlike previously identified parameters, damping effects are less straightforward to calculate. Wind tunnel testing or computer-aided computational fluid dynamics tests can be used to identify the parameters. Another option, and the chosen one for this project, is dynamic testing. For this test, the MoCap Lab and the Vicon System in the lab ( TECH 189) are used to accurately track the vehicle's state during flight tests. The Vicon system uses a series of infrared cameras and, when properly calibrated, can accurately provide dynamic measurements down to 0.017 mm on average [source]. During testing, motor inputs were recorded alongside Vicon data measurements. After running multiple free-flight test trials, the data reduction was performed using MATLAB to calculate the damping coefficient at each time step. The median of this data was then used to represent the damping coefficient for a singular test, and then the average of all trials was taken. This experiment resulted in the following data for the damping coefficient.  </p> <p> </p> <p>It should also be noted that, ideally, single DOF tests would be performed at constant velocities. Thus, eliminating acceleration and thereby mass effects, and also minimizing any potential coupling effects between the parameters. However, due to limited control, all 6 DOF cannot be independently controlled and achieving a constant velocity in an enclosed space can be difficult. Therefore, it was assumed that various flight tests would be sufficient for the calculation.  </p>"},{"location":"#result","title":"Result:","text":"<p>Experimental tests were performed to validate the identified model. In these tests simple trajectory experiments were performed in the Lab and Vicon data was recorded alongside motor inputs. The motor inputs were then extracted and used as inputs to the dynamic model. Making use of the forward kinematics of the system, the theoretical calculation of position was compared to the experimental position measurement. Figure 6 illustrates theoretical and experimental measurements for a \u2018forward only\u2019 motion, while Figure 7 illustrates the results for a \u2018vertical only\u2019 motion.</p> <p> </p> <p> </p> <p>Model validation through experimentation authenticates the similarities between LTAVs and UUVs. As such, it can be reasonably stated that conventional UUV dynamic equations can be used to represent the behavior of an LTAV.  </p>"},{"location":"#34-sensor-integration","title":"3.4. Sensor Integration","text":"<p>Sensor integration in the project adopts a comprehensive, multi-layered approach to ensure effective utilization of data from each sensor across all development stages. The team aims to implement a modular architecture in the codebase where sensor data is published on dedicated ROS2 topics. Each sensor continuously streams its specific measurements, facilitating both independent processing and integrated sensor fusion.  </p> <p>The code utilizes a hierarchical data processing pipeline:  </p> <ol> <li>Data Acquisition Layer: Handles raw sensor input collection at appropriate sampling rates.</li> <li>Filtering Layer: Applies Kalman filtering and other noise reduction techniques.</li> <li>Fusion Layer: Combines multiple sensor inputs for comprehensive state estimation.</li> <li>Decision Layer: Processes fused data to inform navigation and control decisions.</li> </ol>"},{"location":"#35-data-collection-process","title":"3.5. Data Collection Process","text":"<p>During testing, the team will validate individual sensor outputs using ROS2 tools like rqt_plot and ros2 topic echo. This methodical approach ensures each sensor is correctly calibrated and operating within expected parameters. The testing protocol includes:</p> <ul> <li>Sensor Isolation Tests: Verifying each sensor's accuracy and reliability independently.</li> <li>Calibration Verification: Ensuring sensors maintain accuracy across varying conditions.</li> <li>Interference Testing: Identifying potential cross-sensor interference issues.</li> <li>Data Consistency Checks: Confirming consistent readings under controlled conditions.</li> <li>Latency Measurement: Quantifying processing delays for time-sensitive applications.</li> </ul> <p>These tests will be conducted in controlled indoor environments and field trials, allowing us to observe how sensor data influences the system's stability, localization accuracy, and obstacle detection capabilities in diverse settings.  </p> <p>In the final demonstration, real-time integration of sensor data will be paramount. Each sensor will play a specific role in enabling autonomous operation:  </p> <ul> <li>IMU: Provides orientation control with 6-DoF motion tracking at 200Hz.</li> <li>Barometer: Maintains precise altitude control through atmospheric pressure monitoring.</li> <li>Raspberry Pi Camera: Supports visual goal detection and obstacle recognition at 30fps.</li> </ul> <p> Figure 2: BLIMP Sensor Connection Block Diagram</p>"},{"location":"#36-object-detection-and-model-fitting","title":"3.6. Object Detection and Model Fitting","text":"<p>The team trained a custom YOLOv5s model to detect olive-colored balloons, which are the objects of interest.  </p> <p>Training details: - Platform: Roboflow + local training - Dataset: Collected in variable lighting/backgrounds - mAP@0.5: ~96% - Inference time: ~20ms (GPU), &lt;100ms (CPU)</p> <p>Upon detection, the model calculates the centroid of the balloon and converts it to relative pixel coordinates, using the frame's center as the origin. This enables both quadrant classification (<code>Q1\u2013Q4</code>) and distance estimation.  </p> <p> Figure 1: Real-time detection of the green target balloon using a custom-trained YOLOv5 model with quadrant-based localization. </p> <p>The team used the object's position relative to the center to command differential drive actions. As the object nears the center, the blimp slows down and halts when it reaches a defined \"success zone.\"  </p>"},{"location":"#37-validation-and-success-metrics","title":"3.7. Validation and Success Metrics","text":"<p>GUI Mockup for ROS-based Sensor Visualization</p> <p>The ROS-based GUI can be enhanced to include real-time streaming of sensor topics, providing a comprehensive visualization tool for debugging and monitoring system performance. The interface should consist of:</p> <ul> <li>Live Sensor Data Feeds: Displaying IMU, barometer, and camera outputs.</li> <li>Real-Time 3D Mapping: Visualization of detected obstacles and planned trajectories.</li> <li>System Status Indicators: Battery levels, motor commands, and control mode (manual/autonomous).</li> <li>Logging &amp; Playback: Time-stamped data recording for post-mission analysis.</li> </ul> <p>The GUI is built using a ROSBridge Websocket that subscribes to all the sensor data. </p> <p>These updates ensure that the sensor fusion and autonomy modules operate efficiently, providing real-time insights into the BLIMP's state during both testing and deployment.  </p> <p>This comprehensive sensor array will drive the autonomous control loops, enabling adaptive trajectory planning and responsive mode switching between manual and autonomous control. The demonstration will showcase how the fusion algorithms synthesize this diverse data to create a cohesive understanding of the environment, allowing the BLIMP to navigate reliably in dynamic, real-world scenarios.  </p>"},{"location":"#38-interface-development","title":"3.8. Interface Development","text":"<p>The behavior of the BLIMP will be influenced through a sophisticated dual-mode control strategy that balances user input with autonomous decision-making. The team has designed:</p> <ol> <li> <p>Manual Control Mode: Users can precisely adjust the robot's throttle, direction, and altitude via a joystick interface, allowing for direct control during setup, testing, and emergencies.</p> </li> <li> <p>Autonomous Mode: The robot will employ advanced trajectory planning algorithms based on goal positions detected by the onboard camera. The system will continuously process sensor data to maintain a stable flight while navigating toward objectives.</p> </li> <li> <p>Mode Switching: A dedicated joystick button will facilitate seamless transitions between manual and autonomous operations, ensuring operators can quickly regain control if needed.</p> </li> <li> <p>Emergency Override: Safety-critical sensors will trigger automatic responses regardless of current mode, such as obstacle avoidance maneuvers or altitude corrections.</p> </li> </ol> <p>This hybrid approach provides flexibility while maintaining safety and mission objectives.</p> <p>The team is developing a comprehensive GUI based on ROS that serves multiple functions:</p> <ol> <li> <p>Real-time Data Visualization: </p> <ul> <li>Sensor data dashboards displaying IMU, barometer, GPS, camera, sonar, and LiDAR outputs.</li> <li>Interactive 3D map showing current position, trajectory, and detected obstacles.</li> <li>System status indicators including battery levels, motor outputs, and connection quality. </li> </ul> </li> <li> <p>Control Interface:</p> <ul> <li>Mode selection panel (manual/autonomous).</li> <li>Manual control input visualization.</li> <li>Goal setting and waypoint management tools.</li> <li>Parameter adjustment sliders for tuning control algorithms.</li> </ul> </li> <li> <p>Data Logging and Analysis:</p> <ul> <li>Comprehensive time-stamped data recording of all sensor inputs.</li> <li>Performance metrics tracking, including stability measures and navigation accuracy.</li> <li>Export functionality for post-mission analysis.</li> <li>Replay capabilities for reviewing flight data.</li> </ul> </li> </ol> <p>This integrated interface will not only enable immediate interaction during operation but also support detailed post-mission analysis and system refinement.</p>"},{"location":"#4-project-processworkflow","title":"4. Project Process/Workflow","text":"<p>The workflow involved weekly iteration and testing across three verticals: hardware integration, software development, and sensor-based control logic. Key workflow highlights:</p> <ul> <li>Early-stage testing of sensors: IMU, barometer, and camera modules.</li> <li>Development of a modular ROS 2 architecture.</li> <li>Collection and labeling of balloon images for training.</li> <li>Training of YOLOv5 using Roboflow-annotated data.</li> <li>Integration of detection results into a quadrant mapping logic node.</li> <li>Real-world validation through indoor tethered testing.</li> </ul> <p>We followed a tight loop of code-deploy-test-debug with each sensor modality until full system integration.</p>"},{"location":"#5-system-tradeoffs-technical-considerations","title":"5. System Tradeoffs &amp; Technical Considerations","text":"<ul> <li> <p>YOLOv5 vs. Color Thresholding: We opted for a trained YOLOv5 model instead of simple blob detection. While this required more up-front work (data collection and training), it provided robustness in variable lighting.</p> </li> <li> <p>IMU + Barometer Fusion: We avoided using GPS or LiDAR for simplicity. The decision was made to trade positional accuracy for faster, more reactive control using sensor fusion of onboard IMU and barometric pressure.</p> </li> <li> <p>Differential Drive Control: Instead of complex motion planning, we used a direct mapping from image-space offset to control output. This significantly reduced computation time and control latency.</p> </li> <li> <p>ROS 2 Simplicity: We avoided custom middleware and kept communication to ROS 2 topics and standard packages, minimizing overhead.</p> </li> </ul>"},{"location":"#6-what-did-we-learn","title":"6. What did we learn?","text":"<ul> <li>Sensor Fusion: Hands-on experience fusing noisy sensor data taught us the importance of calibration and filtering. We learned to work with real-world imperfections in IMU/barometer readings.</li> <li>Training Computer Vision Models: We developed a better understanding of labeling strategies, data augmentation, and how inference performance varies between GPU and CPU.</li> <li>ROS 2 Practices: Designing modular ROS 2 nodes helped us better grasp the advantages of loosely coupled systems.</li> <li>Iterative Debugging: The importance of visualizing intermediate outputs (OpenCV overlays, topic echo, rqt_graph) became apparent, allowing us to fix bugs faster.</li> </ul>"},{"location":"#7-future-considerations","title":"7. Future Considerations","text":"<ul> <li>PID Control for Smoothness: Our current control is reactive. Adding PID loops will help dampen overshoot and allow finer movement.</li> <li>3D Localization: Incorporating a forward-facing range sensor could allow for estimating object depth, enabling true 3D navigation.</li> <li>Fallback Strategies: Implementing lost-object recovery and search behavior would make the system more resilient.</li> <li>State Machine Architecture: Transitioning from basic logic to a hierarchical state machine could improve scalability and robustness.</li> <li>Model Optimization: Quantization or pruning of YOLOv5 could allow faster inference on low-power devices.</li> <li>Outdoor Capability: With GPS integration and larger test environments, we could extend the system for outdoor deployments.</li> </ul>"},{"location":"#8-project-timeline","title":"8. Project Timeline","text":"Week Hardware Software Testing Status 7 Set up core hardware components Create basic user interface Test basic sensors (IMU, GPS) \u2705 8 Test power systems and batteries Develop interface mockups Install and test distance sensors \u2705 9 Test motors and speed controllers Connect interface to ROS Begin data collection from sensors \u2705 10 Extend hardware setup, wiring Add real-time sensor displays Calibrate initial sensors \u2705 11 Complete system integration Set up data storage + logging Begin combining sensor inputs \u2705 12 Start motor tuning + prop checks Add map view to interface Initial static testing of camera \u2705 13 Prep for flight &amp; safety checks Finalize ROS visualization setup Validate sensor fusion \u2705 14 Conduct initial flight tests \u2708\ufe0f Basic control tuning via interface Monitor live telemetry, adjust logs \u2705 15 Refine hardware from flight data UI polish &amp; final features Autonomous flight path tuning \u2705 16 Final fixes and mounting upgrades Bugfix and code cleanup Final demo rehearsal &amp; testing \u2705 <p>Legend: </p> <ul> <li>\u2705 = Completed  </li> <li>\ud83d\udd04 = In Progress  </li> <li>\ud83d\udd52 = Pending  </li> </ul>"},{"location":"Data/","title":"Data","text":""},{"location":"Data/#experimental-data","title":"Experimental Data","text":"<p>(calibrations\u2026.)</p>"},{"location":"Data/#data-format-and-content","title":"Data Format and Content","text":""},{"location":"Data/#data-filtering","title":"Data Filtering","text":"<p>Effective autonomy depends on reliable sensor input. Here\u2019s how the team will manage each:</p>"},{"location":"Data/#camera-vision-input","title":"Camera (Vision Input)","text":"<ul> <li>Contrast Enhancement: Histogram equalization and HSV color space for better robustness under changing lighting.</li> <li>Filtering: Median filtering and Gaussian blurring to suppress noise.</li> <li>Region of Interest (ROI): Focus processing on the central region to improve performance.</li> </ul>"},{"location":"Data/#imu","title":"IMU","text":"<ul> <li>Sensor Fusion: Combining gyro, accel, and mag data using complementary filters.</li> <li>Filtering: Low-pass filtering for accel and magnetometer data; high-pass to correct gyro drift.</li> </ul>"},{"location":"Data/#servos","title":"Servos","text":"<ul> <li>PID Controllers (planned): For consistent and smooth motion.</li> <li>Encoder Feedback: To adjust velocity in real time.</li> <li>Noise Filtering: Reduce jitter in encoder readings via smoothing.</li> </ul>"},{"location":"Data/#decision-making-overview","title":"Decision-Making Overview","text":"<p>Sensor data feeds into both short-term (control) and long-term (behavioral) decisions:</p>"},{"location":"Data/#low-level-decisions","title":"Low-Level Decisions","text":"<ul> <li>Set PWM/servo speeds based on position errors</li> <li>Adjust the throttle for altitude</li> <li>Stabilize yaw using IMU orientation</li> <li>Apply real-time filters for smoother control</li> </ul>"},{"location":"Data/#high-level-decisions","title":"High-Level Decisions","text":"<ul> <li>Use YOLO to detect and localize the balloon</li> <li>Estimate relative direction and command movement</li> <li>Track proximity to frame center and decide when to stop</li> <li>Manage behavioral states: searching \u2192 tracking \u2192 stop</li> </ul>"},{"location":"Data/#updated-sensor-flowchart","title":"Updated Sensor Flowchart","text":"<pre><code>graph TD\n    Camera --&gt;|/image_raw| YOLOv5_Detector\n    YOLOv5_Detector --&gt;|/blimp/detected_position| QuadrantLogic\n    IMU --&gt;|/imu_data| InvKine\n    Barometer --&gt;|/barometer_data| InvKine\n    QuadrantLogic --&gt;|/balloon_input| InvKine\n    InvKine --&gt;|/forces| F_to_ESC\n    F_to_ESC --&gt;|/ESC_balloon_input| ModeSwitch\n    Joy --&gt;|/ESC_Manual_input| ModeSwitch\n    ModeSwitch --&gt;|/ESC_input| ESC_Driver\n    ESC_Driver --&gt; Actuators\n</code></pre>"},{"location":"Data/#data-visualization-examples","title":"Data Visualization Examples\ud83d\udcf9 Balloon Tracking Demo","text":"<p>The current graphical user interface (GUI) is designed using a ROS 2 WebSocket-based architecture to enable real-time monitoring and control of the aerial robot. This approach was selected due to performance limitations observed during early tests, particularly latency and bandwidth issues with direct streaming from the onboard Raspberry Pi using tools like RQT or VNC.  </p> <p>To overcome these constraints, the team developed a lightweight, browser-accessible GUI that interfaces with ROS 2 via rosbridge_server and roslibjs. This setup allows for JSON-based communication over WebSockets, facilitating seamless interaction with ROS topics, services, and messages.  </p> <p>Technologies used: </p> <ul> <li>HTML/CSS/JavaScript: For layout, styling, and interactivity</li> <li>roslibjs: For WebSocket communication with ROS 2</li> <li>rosbridge_server: Acts as a middleware bridge translating ROS messages into JSON</li> </ul> <p>Features supported: </p> <ul> <li>Live video streaming from the onboard camera (via MJPEG)</li> <li>Real-time visualization of IMU data and telemetry</li> <li>Manual control inputs using buttons or joystick events</li> <li>System diagnostics and status monitoring</li> </ul> <p>By offloading the GUI rendering to a browser, onboard compute resources are reserved for mission-critical ROS 2 tasks. This results in a responsive, low-latency interface suitable for remote deployment.  </p> <p> Figure 3: Real-time sensor and control interface for the BLIMP, developed using roslibjs and rosbridge_server over WebSockets.</p>    Your browser does not support the video tag.  ## Summary  The system is now capable of:    - Detecting and tracking a balloon in real time - Navigating toward the balloon using differential drive - Halting autonomously upon reaching the success zone - Publishing and processing filtered IMU sensor data  Next Steps:    - Fully synchronize the GUI with detection and motor control nodes - Implement PID control for smoother and more stable navigation - Add fallback mechanisms when the target is lost - Fine-tune success detection thresholds - Log and evaluate system performance using rosbag"},{"location":"Gallery/","title":"\ud83c\udfa5 BLIMP Demonstration Gallery","text":"<p>This gallery showcases various modules of the BLIMP platform including altitude control, manual joystick input, and real-time object detection using YOLO.</p>"},{"location":"Gallery/#altitude-control","title":"\ud83d\udccc Altitude Control","text":"Your browser does not support the video tag.      <p>Altitude ControllerMaintains vertical position using IMU + Barometer.</p>        Your browser does not support the video tag.      <p>Altitude Control UIReal-time visualization of altitude feedback loop.</p>"},{"location":"Gallery/#object-detection-gui-integration","title":"\ud83d\udccc Object Detection &amp; GUI Integration","text":"Your browser does not support the video tag.      <p>YOLOv5 3D Estimation GUI ViewLive object detection using a trained YOLO model with 3D Estimation GUI overlays.</p>        Your browser does not support the video tag.      <p>Object Detection + GUIDisplays detection output, quadrant logic, and system status.</p>"},{"location":"Gallery/#manual-and-autonomous-control","title":"\ud83d\udccc Manual and Autonomous Control","text":"Your browser does not support the video tag.      <p>Manual Control TestJoystick-driven navigation and rotation using ROS2 teleop.</p>        Your browser does not support the video tag.      <p>YOLO + Control LoopBLIMP autonomously tracks and moves toward detected object.</p>"},{"location":"ROS%20Integration/","title":"ROS Integration","text":""},{"location":"ROS%20Integration/#code-repository","title":"Code Repository","text":"<p>https://github.com/RAS598-2025-S-Team03/BLIMP-Packages</p>"},{"location":"ROS%20Integration/#overview-of-code","title":"Overview of Code:","text":""},{"location":"ROS%20Integration/#detect_cpp","title":"Detect_cpp","text":"<p>The <code>detect_cpp</code> node is located at <code>wvu_blimps_ros2_src/sensors_cpp/src/detect_node.cpp</code> and is implemented in C++. It subscribes to the <code>/joy</code> topic and publishes camera-based detections to <code>/cam_data</code>. This node captures images from the onboard camera and operates in two modes: balloon detection and goal detection, toggled using the Xbox controller. Balloon detection uses HSV color space to find the densest region matching the target color. If the region exceeds a minimum radius, the center is averaged over several frames before publishing. Goal detection uses Canny edge detection and Hough line transforms to calculate the goal\u2019s midpoint based on detected vertical and horizontal edges.</p>"},{"location":"ROS%20Integration/#esc_driver","title":"Esc_driver","text":"<p>The <code>esc_driver</code> Python node is found in <code>wvu_blimps_ros2_src/controls/controls/esc_driver.py</code>. It subscribes to the <code>/joy</code> topic and uses <code>pigpio</code> to control motor PWM signals. It translates mode-switch inputs into ESC commands and manages communication with the controller over Bluetooth.</p>"},{"location":"ROS%20Integration/#f_to_esc","title":"F_to_esc","text":"<p>The <code>F_to_esc</code> node, written in C++ at <code>wvu_blimps_ros2_src/sensors_cpp/src/force_to_ESC_input.cpp</code>, subscribes to force vectors and publishes motor commands. It performs a pseudo-inverse transformation to convert forces in the body frame into ESC inputs. Dependencies include Eigen for matrix math and custom interfaces for handling Cartesian and PWM data.</p>"},{"location":"ROS%20Integration/#game_controller_node","title":"Game_controller_node","text":"<p>This node reads Xbox controller inputs and publishes them to <code>/joy</code>. It is built on the standard ROS 2 Joy package and is responsible for providing manual input data to various control nodes.</p>"},{"location":"ROS%20Integration/#inv_kine","title":"Inv_kine","text":"<p>The <code>inv_kine</code> node resides in <code>wvu_blimps_ros2_src/sensors_cpp/src/inv_kine.cpp</code>. It takes input from <code>/imu_data</code>, <code>/barometer_data</code>, and <code>/balloon_input</code> to estimate forces acting on the blimp. Using these inputs, it publishes force vectors via the <code>/forces</code> topic. It incorporates a dynamic model that uses IMU orientation, barometer-based vertical velocity, and visual feedback to determine control outputs.</p>"},{"location":"ROS%20Integration/#joy_to_esc","title":"Joy_to_esc","text":"<p>Located in <code>wvu_blimps_ros2_src/controls/controls/joy_to_esc_input.py</code>, this Python node directly maps joystick commands to ESC inputs for manual control. It serves as a simpler alternative to autonomous control logic when operating in manual mode.</p>"},{"location":"ROS%20Integration/#mode_switch","title":"Mode_switch","text":"<p>The <code>mode_switch</code> node selects between manual and autonomous control. It listens to <code>/ESC_Manual_input</code>, <code>/ESC_balloon_input</code>, and <code>/joy</code>, and publishes the final ESC command to <code>/ESC_input</code>. It is central to toggling between pilot input and autonomous operation during testing.</p>"},{"location":"ROS%20Integration/#pi_controller","title":"Pi_controller","text":"<p>Implemented in C++ and found at <code>wvu_blimps_ros2_src/sensors_cpp/src/test.cpp</code>, this node acts as a PI controller. It receives camera and barometer data and outputs control commands to the inverse kinematic model. Parameters include proportional and integral gains for yaw and vertical acceleration, as well as camera frame center coordinates for error correction.</p>"},{"location":"ROS%20Integration/#read_altitude","title":"Read_altitude","text":"<p>This Python node at <code>wvu_blimps_ros2_src/sensors/sensors/barometer.py</code> reads atmospheric pressure using an Adafruit BMP390 and estimates altitude. It publishes data on the <code>barometer_data</code> topic and accounts for sea level pressure calibration to improve accuracy.</p>"},{"location":"ROS%20Integration/#read_imu","title":"Read_imu","text":"<p>The <code>read_imu</code> node, written in Python, reads data from the Adafruit BNO055 IMU. It measures Euler angles, linear acceleration, and angular velocity, publishing all data to the <code>/imu_data</code> topic. This sensor information supports real-time state estimation for the blimp.</p>"},{"location":"ROS%20Integration/#system-prerequisites","title":"System Prerequisites","text":"<p>Before deploying the autonomous blimp system, ensure the following hardware and software components are properly installed and configured:</p> <ul> <li>A Raspberry Pi (with SSH access enabled)</li> <li>Required Python libraries including <code>rclpy</code>, <code>pigpio</code>, <code>adafruit_bno055</code>, <code>adafruit_bmp3xx</code>, <code>numpy</code>, and related dependencies</li> <li><code>pigpio</code> daemon running for motor PWM control</li> <li>ROS 2 Humble (or compatible ROS 2 distribution) fully set up</li> <li>Xbox controller paired to the Raspberry Pi via Bluetooth</li> <li>Access to the <code>BLIMP-Packages</code> ROS 2 workspace cloned from the repository:   https://github.com/RAS598-2025-S-Team03/BLIMP-Packages</li> </ul>"},{"location":"ROS%20Integration/#deployment-and-launch-instructions","title":"Deployment and Launch Instructions","text":"<p>To run the complete autonomous system on the Raspberry Pi:</p> <ol> <li> <p>Navigate to the launch directory within the ROS 2 workspace: <code>cd ~/ros2_ws/src/wvu_blimps_ros2_src/launch</code></p> </li> <li> <p>Initialize the game controller node to verify that joystick inputs are being received: <code>ros2 run joy game_controller_node</code></p> </li> <li> <p>Arm the ESCs before flight using the custom arming script: <code>python3 Arming.py</code></p> </li> <li> <p>Launch all nodes together using the master launch file: <code>ros2 launch Updated_launch.py</code></p> </li> </ol> <p>These steps start all relevant ROS 2 nodes for sensor input, control logic, mode switching, and actuator commands. Be sure to confirm that each node is publishing correctly by checking the appropriate topics using <code>ros2 topic list</code> or <code>rqt_graph</code>.</p>"},{"location":"static/node_modules/mathjax/","title":"MathJax","text":""},{"location":"static/node_modules/mathjax/#beautiful-math-in-all-browsers","title":"Beautiful math in all browsers","text":"<p>MathJax is an open-source JavaScript display engine for LaTeX, MathML, and AsciiMath notation that works in all modern browsers.  It was designed with the goal of consolidating the recent advances in web technologies into a single, definitive, math-on-the-web platform supporting the major browsers and operating systems.  It requires no setup on the part of the user (no plugins to download or software to install), so the page author can write web documents that include mathematics and be confident that users will be able to view it naturally and easily.  Simply include MathJax and some mathematics in a web page, and MathJax does the rest.</p> <p>Some of the main features of MathJax include:</p> <ul> <li> <p>High-quality display of LaTeX, MathML, and AsciiMath notation in HTML pages</p> </li> <li> <p>Supported in most browsers with no plug-ins, extra fonts, or special   setup for the reader</p> </li> <li> <p>Easy for authors, flexible for publishers, extensible for developers</p> </li> <li> <p>Supports math accessibility, cut-and-paste interoperability, and other   advanced functionality</p> </li> <li> <p>Powerful API for integration with other web applications</p> </li> </ul> <p>See http://www.mathjax.org/ for additional details about MathJax, and https://docs.mathjax.org for the MathJax documentation.</p>"},{"location":"static/node_modules/mathjax/#mathjax-components","title":"MathJax Components","text":"<p>MathJax version 3 uses files called components that contain the various MathJax modules that you can include in your web pages or access on a server through NodeJS.  Some components combine all the pieces you need to run MathJax with one or more input formats and a particular output format, while other components are pieces that can be loaded on demand when needed, or by a configuration that specifies the pieces you want to combine in a custom way.  For usage instructions, see the MathJax documentation.</p> <p>Components provide a convenient packaging of MathJax's modules, but it is possible for you to form your own custom components, or to use MathJax's modules directly in a node application on a server.  There are web examples showing how to use MathJax in web pages and how to build your own components, and node examples illustrating how to use components in node applications or call MathJax modules directly.</p>"},{"location":"static/node_modules/mathjax/#whats-in-this-repository","title":"What's in this Repository","text":"<p>This repository contains only the component files for MathJax, not the source code for MathJax (which are available in a separate MathJax source repository).  These component files are the ones served by the CDNs that offer MathJax to the web.  In version 2, the files used on the web were also the source files for MathJax, but in version 3, the source files are no longer on the CDN, as they are not what are run in the browser.</p> <p>The components are stored in the <code>es5</code> directory, and are in ES5 format for the widest possible compatibility.  In the future, we may make an <code>es6</code> directory containing ES6 versions of the components.</p>"},{"location":"static/node_modules/mathjax/#installation-and-use","title":"Installation and Use","text":""},{"location":"static/node_modules/mathjax/#using-mathjax-components-from-a-cdn-on-the-web","title":"Using MathJax components from a CDN on the web","text":"<p>If you are loading MathJax from a CDN into a web page, there is no need to install anything.  Simply use a <code>script</code> tag that loads MathJax from the CDN.  E.g.,</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See the MathJax documentation, the MathJax Web Demos, and the MathJax Component Repository for more information.</p>"},{"location":"static/node_modules/mathjax/#hosting-your-own-copy-of-the-mathjax-components","title":"Hosting your own copy of the MathJax Components","text":"<p>If you want to host MathJax from your own server, you can do so by installing the <code>mathjax</code> package using <code>npm</code> and moving the <code>es5</code> directory to an appropriate location on your server:</p> <pre><code>npm install mathjax@3\nmv node_modules/mathjax/es5 &lt;path-to-server-location&gt;/mathjax\n</code></pre> <p>Note that we are still making updates to version 2, so include <code>@3</code> when you install, since the latest chronological version may not be version 3.</p> <p>Alternatively, you can get the files via GitHub:</p> <pre><code>git clone https://github.com/mathjax/MathJax.git mj-tmp\nmv mj-tmp/es5 &lt;path-to-server-location&gt;/mathjax\nrm -rf mj-tmp\n</code></pre> <p>Then (in either case) you can use a script tag like the following:</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"&lt;url-to-your-site&gt;/mathjax/tex-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>where <code>&lt;url-to-your-site&gt;</code> is replaced by the URL to the location where you moved the MathJax files above.</p> <p>See the documentation for details.</p>"},{"location":"static/node_modules/mathjax/#using-mathjax-components-in-a-node-application","title":"Using MathJax components in a node application","text":"<p>To use MathJax components in a node application, install the <code>mathjax</code> package:</p> <pre><code>npm install mathjax@3\n</code></pre> <p>(we are still making updates to version 2, so you should include <code>@3</code> since the latest chronological version may not be version 3).</p> <p>Then require <code>mathjax</code> within your application:</p> <pre><code>require('mathjax').init({ ... }).then((MathJax) =&gt; { ... });\n</code></pre> <p>where the first <code>{ ... }</code> is a MathJax configuration, and the second <code>{ ... }</code> is the code to run after MathJax has been loaded.  E.g.</p> <pre><code>require('mathjax').init({\nloader: {load: ['input/tex', 'output/svg']}\n}).then((MathJax) =&gt; {\nconst svg = MathJax.tex2svg('\\\\frac{1}{x^2-1}', {display: true});\nconsole.log(MathJax.startup.adaptor.outerHTML(svg));\n}).catch((err) =&gt; console.log(err.message));\n</code></pre> <p>Note: this technique is for node-based application only, not for browser applications.  This method sets up an alternative DOM implementation, which you don't need in the browser, and tells MathJax to use node's <code>require()</code> command to load external modules.  This setup will not work properly in the browser, even if you webpack it or bundle it in other ways.</p> <p>See the documentation and the MathJax Node Repository for more details.</p>"},{"location":"static/node_modules/mathjax/#reducing-the-size-of-the-components-directory","title":"Reducing the Size of the Components Directory","text":"<p>Since the <code>es5</code> directory contains all the component files, so if you are only planning one use one configuration, you can reduce the size of the MathJax directory by removing unused components. For example, if you are using the <code>tex-chtml.js</code> component, then you can remove the <code>tex-mml-chtml.js</code>, <code>tex-svg.js</code>, <code>tex-mml-svg.js</code>, <code>tex-chtml-full.js</code>, and <code>tex-svg-full.js</code> configurations, which will save considerable space.  Indeed, you should be able to remove everything other than <code>tex-chtml.js</code>, and the <code>input/tex/extensions</code>, <code>output/chtml/fonts/woff-v2</code>, <code>adaptors</code>, <code>a11y</code>, and <code>sre</code> directories.  If you are using the results only on the web, you can remove <code>adaptors</code> as well.</p> <p>If you are not using A11Y support (e.g., speech generation, or semantic enrichment), then you can remove <code>a11y</code> and <code>sre</code> as well (though in this case you may need to disable the assistive tools in the MathJax contextual menu in order to avoid MathJax trying to load them when they aren't there).</p> <p>If you are using SVG rather than CommonHTML output (e.g., <code>tex-svg.js</code> rather than <code>tex-chtml.js</code>), you can remove the <code>output/chtml/fonts/woff-v2</code> directory.  If you are using MathML input rather than TeX (e.g., <code>mml-chtml.js</code> rather than <code>tex-chtml.js</code>), then you can remove <code>input/tex/extensions</code> as well.</p>"},{"location":"static/node_modules/mathjax/#the-component-files-and-pull-requests","title":"The Component Files and Pull Requests","text":"<p>The <code>es5</code> directory is generated automatically from the contents of the MathJax source repository.  You can rebuild the components using the command</p> <pre><code>npm run make-es5 --silent\n</code></pre> <p>Note that since the contents of this repository are generated automatically, you should not submit pull requests that modify the contents of the <code>es5</code> directory.  If you wish to submit a modification to MathJax, you should make a pull request in the MathJax source repository.</p>"},{"location":"static/node_modules/mathjax/#mathjax-community","title":"MathJax Community","text":"<p>The main MathJax website is http://www.mathjax.org, and it includes announcements and other important information.  A MathJax user forum for asking questions and getting assistance is hosted at Google, and the MathJax bug tracker is hosted at GitHub.</p> <p>Before reporting a bug, please check that it has not already been reported.  Also, please use the bug tracker (rather than the help forum) for reporting bugs, and use the user's forum (rather than the bug tracker) for questions about how to use MathJax.</p>"},{"location":"static/node_modules/mathjax/#mathjax-resources","title":"MathJax Resources","text":"<ul> <li>MathJax Documentation</li> <li>MathJax Components</li> <li>MathJax Source Code</li> <li>MathJax Web Examples</li> <li>MathJax Node Examples</li> <li>MathJax Bug Tracker</li> <li>MathJax Users' Group</li> </ul>"}]}